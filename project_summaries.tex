\subsection*{\href{https://www.youtube.com/watch?v=Z7YiruHv3eE}{Project 1: Multi-objective Benchmarking of Dragonfly against BoTorch}}

This research project compared the performance of two BO packages, Dragonfly\cite{JMLRdragonfly} and BoTorch\cite{balandat2020botorch}, in a multi-objective optimization context. The researchers optimized the Branin-Currin test function with added noise, conducting 20 trials of BO over different random seeds. Each trial used a batch size of 1 and consisted of 20 iterations, evaluating 20 candidates per iteration. The results showed that BoTorch's acquisition functions, particularly the q-noisy expected hypervolume improvement (qNEHVI)\cite{daulton2021parallel}, outperformed Dragonfly's acquisition function. The qNEHVI acquisition function achieved the lowest log hypervolume difference on average across the trials, while Dragonfly's acquisition function struggled to surpass a randomly generated set of candidates. Visualization of the average Pareto front produced by different acquisition functions further demonstrated BoTorch's superiority, with its acquisition functions covering a larger portion of the Pareto front compared to Dragonfly. The researchers concluded that BoTorch outperformed Dragonfly within the scope of their study, but noted the need for further investigation using different batch sizes, input and output constraints, and test functions simulating chemical reactions to apply the findings to material design problems.
 \subsection*{\href{https://www.youtube.com/watch?v=pegJumJEOsE}{Project 2: Long-run Behaviour of Multi-fidelity Bayesian Optimisation}}

This research project investigates the long-term performance of multifidelity Bayesian optimization (MFBO) methods\cite{poloczek2017multi} compared to single-fidelity Bayesian optimization (SFBO) methods. The study focuses on scenarios where two fidelities are available: an accurate but expensive target fidelity that fully represents the objective, and a less accurate but cheaper low fidelity. The researchers observed that while MFBO methods initially outperform SFBO up to a budget of approximately 20, they begin to underperform beyond this point. This behavior is particularly evident with linear kernel MFBO methods, where the performance gap becomes more pronounced after a cost of around 60.The project aims to evaluate the factors contributing to the long-term issues of MFBO methods. Specifically, the researchers plan to investigate the impact of kernel choice, frequency of low fidelity queries, and acquisition function selection on MFBO performance. By analyzing these aspects, the study seeks to identify potential improvements for MFBO methods to enhance their applicability in real-life optimization tasks. The ultimate goal is to develop MFBO approaches that can maintain their performance advantage over SFBO methods in the long run, leveraging the cost-effectiveness of low fidelity queries while achieving superior optimization results.
 \subsection*{\href{https://www.youtube.com/watch?v=WkGfShRSYW4}{Project 3: Take Your Time - Improving Optimization Performance Through Greater Investment in ACQF Optimizer Runtime}}

This research project explores an enhancement to standard BO campaigns by introducing a random seed approach to improve optimization outcomes. The team from the University of Utah's Material Science and Engineering department investigated the variability in BO campaigns when starting from the same initial data points. The key innovation in this work is a custom approach to calculating the acquisition function. Instead of computing it once per iteration, as is standard in most BO methods\cite{shahriari2015taking, snoek2012practical}, the researchers calculate the acquisition function multiple times using different random seeds. They then select the highest-performing result at each iteration. This method, termed "random retries Optimizer", showed significant improvements in optimization performance, particularly in avoiding local optima. The researchers demonstrated that this approach consistently achieves near-optimal performance among the possible campaigns stemming from the initial data points. While this method requires additional computational time for the multiple acquisition function calculations, the performance gains appear to outweigh the increased computational cost. The team is continuing to investigate questions such as the trade-off between additional compute time and performance improvements, as well as how the method performs across different types of optimization problems, including simpler ones compared to the challenging example presented.
 \subsection*{\href{https://www.youtube.com/watch?v=Qr2cz5lxM64&ab_channel=ArifinSan}{Project 4: SimpleGPT-BO, Simplified GPT-Powered Bayesian Optimization}}

No summary available.
 \subsection*{\href{https://www.youtube.com/watch?v=znXZhSqFtHg}{Project 5: Comparing Bayesian Optimization Methods Across Multiple Hyperparameters Against Simulated "Human" Decision-making}}

This research project investigated the effectiveness of BO methods that mimic human experimentalist approaches in materials science\cite{muckley2023interpretable}, comparing them to traditional BO techniques and random search baselines\cite{snoek2012practical,bergstra2012random}. The study focused on three key axes: model complexity, number of features, and acquisition function. The researchers found that a simplified "human experimentalist" approach, using automated feature engineering with a linear model, three features, and an exploitative acquisition function, was competitive with more complex random forest models in terms of enhancement factor and acceleration factor. Both the simplified approach and random forest models outperformed random search. Interestingly, for linear models, increasing complexity through additional features or more exploratory acquisition functions led to decreased performance. The main conclusion was that low-dimensional, interpretable models were comparable to traditional BO methods for the datasets and hyperparameter regimes studied. The researchers suggest future work could involve comparing their results to actual human performance and improving their methodology using theoretical approaches.
 \subsection*{\href{https://www.youtube.com/watch?v=RgEbcWIBDn8}{Project 6: Multi-Objective Bayesian Optimization for Transparent Electromagnetic Interference Shielding with Thin-Film Structures}}

This research project focuses on applying multi-objective BO to design transparent electromagnetic interference (EMI) shielding using thin film structures\cite{li2022bayesian}. The primary objectives are to maximize both transmittance and shielding effectiveness simultaneously. The optimization problem involves selecting materials and thicknesses for each layer of the thin film structure, with a search space comprising 12 material choices (Ag, Al, Al$_2$O$_3$, Cr, Ni, Pd, Si$_3$N$_4$, SiO$_2$, Ti, TiN, TiO$_2$, W) and thickness ranges from 5 to 20 nm. The researchers implemented multi-objective BO using random scalarization for the acquisition function via BayesO package\cite{kim2023bayeso}. Gaussian process regression and expected improvement were employed as part of the BO framework. The results demonstrate that the multi-objective BO approach effectively identified the Pareto frontier for the two objectives of transmittance and shielding effectiveness. This suggests that the method is successful in finding optimal trade-offs between transparency and EMI shielding performance for thin film structures, which could have applications in areas such as spacecraft windows where both properties are crucial.
 \subsection*{\href{https://www.youtube.com/watch?v=kIRxGdwmLSY}{Project 7: BayBE One More Time - Exploring Corrosion Inhibitors for Materials Design}}

This research project explored the use of BO for efficiently identifying effective corrosion inhibitors in materials design. The researchers utilized the BO implementation from the BayBE package\cite{fitzner2022baybe} to evaluate its performance on multiple experimental datasets\cite{galvao2022cordata} involving different alloys, primarily aluminum-based. The study found that the choice of molecular descriptor encoding significantly impacted the optimization performance, especially for larger datasets like A1000 and A2024. The SMILES-based approach with Mordred encoding\cite{moriwaki2018mordred} outperformed the other methods for these datasets, while random sampling underperformed. Interestingly, for the A775 dataset, which had a sparse distribution of reported efficiencies and fewer data points, this trend was not observed. The researchers also explored transfer learning capabilities, using information from the A2024 alloy dataset to inform the optimization process for the A1000 dataset. This transfer learning approach outperformed the uninformed method after only 15 iterations, demonstrating its potential for accelerating the discovery of effective corrosion inhibitors in experimental settings.
 \subsection*{\href{https://www.youtube.com/watch?v=5f_UwsfYrc8}{Project 8: BO for Drug Discovery-What is the role of molecular representation?}}

This research project investigated the impact of molecular featurization methods on BO performance for guiding molecular experiments. The team explored alternatives to the commonly used connectivity-based fingerprints, aiming to determine which featurization techniques yield the best results in BO for molecular discovery.The study found that without modifying the features, MACCS\cite{durant2002reoptimization} and RDKit\cite{landrum2013rdkit} featurization methods outperformed the default options presented in previous publications. To address the high dimensionality of molecular features, the researchers employed specialized Gaussian processes and explored random forest surrogates. However, the random forest approach did not improve performance and required more computational resources. Principal Component Analysis (PCA) was successfully used to enable Gaussian processes to handle large molecular featurizations. Notably, physicochemical featurizations like RDKit and Mordred\cite{moriwaki2018mordred} outperformed the previous benchmark winner, MolDQN\cite{zhou2019optimization}. The team's findings were consistent with benchmarks in the BO library but suggested that RDKit could produce even better results than Mordred. Overall, the research highlights the importance of careful featurization selection in BO for molecular discovery, with RDKit emerging as a particularly effective option.
 \subsection*{\href{https://youtu.be/l0aVZDMwIMU}{Project 9: Optimal MOF Selection for CO$_2$ capture using Thompson sampling}}

This research project focuses on optimizing the selection of Metal-Organic Frameworks (MOFs) for carbon capture applications using BO, specifically Thompson Sampling. MOFs are nanoporous materials with high potential for carbon capture\cite{furukawa2013chemistry, heo2020metal}, but their synthesis and characterization are expensive and time-consuming\cite{desantis2017techno}. The goal was to develop an efficient method for identifying high-performing MOF candidates using a small dataset. The team employed a Gaussian Process model trained on the CRAFTED dataset\cite{oliveira2023crafted}, using both RACs features and geometric features of MOFs, with CO$_2$ uptake as the output. Thompson Sampling was used as the acquisition function to select the next best sample based on the posterior distribution, inherently performing a Bayesian update. The results showed that Thompson Sampling was twice as efficient as random sampling in identifying high-performing MOF candidates. This approach is notable for being the first application of Thompson Sampling to MOF candidate selection, and it requires no hyperparameter tuning, making it easily transferable and cost-efficient. The potential impact of this method is significant, as it could accelerate MOF design for direct air capture applications, with a goal of absorbing 30 million tons of CO$_2$, and could also be applied to post-combustion capture scenarios.
 \subsection*{\href{https://www.youtube.com/watch?v=4lFEUixwkE8}{Project 10: Navigating the black box of zeolite synthesis with Bayesian Optimization}}

This research project explores the application of BO to zeolite synthesis, a complex process with significant industrial importance. Zeolites are crystalline materials composed of interconnected silicate and aluminate tetrahedra, widely used as absorbents and catalysts\cite{dusselier2018small}. The synthesis of zeolites involves multiple parameters, including silicon and aluminum sources, organic molecules, water, temperature, and time. The process aims to achieve specific properties such as high crystallinity, large external surface area, and particular silicon-to-aluminum ratios, while also considering economic factors like synthesis temperature, duration, and ingredient concentrations\cite{mallette2024current}. The project presents a GitHub repository containing an introductory text on zeolites and their synthesis, along with a Jupyter notebook demonstrating BO applications. The notebook is divided into two main sections: the first optimizes an analytical dummy function using zeolite synthesis parameters, exploring various aspects of BO including continuous, categorical, and mixed variable types, parameter constraints, and single and multiple objectives. The second section applies BO to propose new experiments based on existing literature data. This approach aims to accelerate the traditionally time-consuming trial-and-error process of zeolite synthesis optimization, potentially reducing costs and improving efficiency in industrial applications.
 \subsection*{\href{https://www.youtube.com/watch?v=HASa3tFLZoI}{Project 11: BlendDS - An intuitive specification of the design space for blends of components}}

This project focuses on developing an interface for BO that bridges the gap between domain experts (i.e. chemists and materials scientists) and machine learning algorithms. The system allows scientists to specify experimental parameters and constraints in natural language, which is then translated into a structured dictionary format by a large language model. This dictionary is subsequently converted into a Python object that can be used for design of experiments and optimization tasks.The key feature of this interface is its ability to generate diverse trial designs for efficient sampling of the experimental space. It incorporates dimensionality reduction techniques to visualize the design space, enabling users to select the most diverse and informative trials. This approach aims to maximize the information gained from each experiment, potentially reducing the number of trials needed to reach optimal results. The project is open-source, inviting contributions and integration with existing BO frameworks.
 \subsection*{\href{https://www.youtube.com/watch?v=jSPGCgH31Hc}{Project 12: Robust GPs for Sustainable Concrete via Bayesian Optimization}}

This project introduces Robust Gaussian Processes (Robust GPs)\cite{altamirano2023robust} to the concrete sciences experiments with Bayesian optimization campaigns. The research addresses the common issue of outliers in concrete science experiments by applying Robust GPs to account for these anomalies. The project provides a tutorial and accompanying code in a GitHub repository, demonstrating how to implement Robust GPs in this specific context. The study utilizes a dataset provided by Meta\cite{ament2310sustainable}, which includes input variables and a target variable representing concrete strength. Initial data analysis reveals potential outliers in the strength variable, with values between 2 and 2,000. The researchers then apply Robust GP to this dataset, achieving good predictive performance. The project culminates in demonstrating how to obtain Bayesian recommendations using the Robust GP as a surrogate model, providing a practical approach for optimizing concrete mixtures while accounting for experimental outliers.
 \subsection*{\href{https://www.youtube.com/watch?v=Cyyj9ySybZE}{Project 13: Interpretability of Bayesian Optimisation Campaigns}}

This research project focused on developing novel methods for interpreting Bayesian Optimization campaigns by incorporating the temporal component often neglected in end-of-campaign analyses. The study utilized data from a self-driving lab experiment aimed at optimizing the conductivity of a coating. The researchers recreated the Gaussian Process (GP) model employed in the original experiment, training it on subsets of data to simulate mid-campaign conditions. Two primary methods were investigated: First, cross-sections of the GP model were taken at various stages of the campaign, holding all but one feature constant. This approach revealed that closer nozzle distances produced better results, aligning with prior beliefs and serving as a potential mid-campaign sense check to validate model alignment with previous experimentation. The second method involved predicting the error of the next sample based on the model trained on data collected up to that point. This analysis showed that performance improvement plateaued around the time the optimal sample was produced, suggesting its potential use as an early stopping criterion if error is monitored throughout the campaign.The research demonstrates the value of incorporating temporal analysis in Bayesian Optimization campaigns, offering insights that could enhance decision-making during experiments. The cross-sectional approach provides a means to validate model behavior against prior knowledge, while the error prediction method could inform stopping criteria, potentially improving efficiency in optimization processes. These techniques offer promising avenues for real-time interpretation and guidance in Bayesian Optimization experiments, particularly in self-driving lab contexts.
 \subsection*{Project 14: Bayesian optimization of likely negative candidates in imbalanced biological datasets}


 \subsection*{\href{https://www.youtube.com/watch?v=utnWbJsObF0}{Project 15: Adaptive Batch Sizes for Bayesian Optimization of Reaction Yield}}

This research project investigates the optimization of batch sizes in BO for chemical reaction yield optimization. The study focuses on determining the optimal batch size (Q) at each step of the BO process, considering both smaller and larger batch sizes. The researchers examine how Q impacts the cost of retraining models, such as large language models, and how it relates to additional experimentation overhead. The project findings indicate that the optimal Q depends on a trade-off between model retraining time and additional experimentation overhead. For a fixed batch size, the researchers observed two minima at Q=3 and Q=7, reflecting the relative importance of Q in different scenarios. With very large batch sizes, the retraining cost had minimal impact on the overall experimental overhead, while experimentation overhead had a significant influence. The study also explored adaptive batch sizing strategies, decreasing batch sizes when performance was good and increasing them as the surrogate model became more trustworthy. The researchers conclude that batch sizes have a crucial effect on BO efficiency in real-world settings, and adaptive batch sizes could effectively balance the trade-off between model retraining and batch sampling effectiveness.
 \subsection*{\href{https://www.youtube.com/watch?v=AbRDOdmafB8}{Project 16: BOPE-GPT, Preference Exploration with the curious AI chemist}}

This research project focused on optimizing the Fischer-Tropsch synthesis process\cite{mahmoudi2017review}, which converts syngas into biofuel hydrocarbons. The optimization problem on the dataset\cite{lozano2008single,chakkingal2022multi} involved four input variables (space time, syngas ratio, temperature, and pressure) and four output variables (carbon conversion, selectivity, methane to paraffins, and olefins). The researchers proposed a novel approach using preferential BO\cite{lin2022preference} with large language models (LLMs) to prioritize outputs based on stakeholder preferences. The methodology employed the typical BO process powered by BoTorch, using Expected Utility as the acquisition function. However, the innovation lay in using an LLM (specifically GPT-4) to perform pairwise comparisons for output prioritization. The results showed that outputs exhibited well-defined optima trade-offs and some degree of monotonicity, which was important for preferential BO. The researchers tested three objective scenarios: optimizing all outputs equally, maximizing only CO conversion, and maximizing three objectives while minimizing olefins. For the first two objectives, the LLM-based preferential BO performed similarly to traditional utility function-based methods. However, for the third, more complex objective, the LLM approach performed worse than the utility function method and only slightly better than random exploration, suggesting limitations in the LLM's ability to handle complex optimization scenarios. The project also included the development of an interactive application for visualizing BO results and running live optimizations.
 \subsection*{\href{https://www.youtube.com/watch?v=5AjwoZtjgOc}{Project 17: Comparative Analysis of Acquisition Functions in Bayesian Optimization for Drug Discovery}}

This project investigates the application of BO techniques directly on molecular fingerprints for drug discovery, focusing on comparing different surrogate models, acquisition functions and small, diverse, unbalanced, and noisy datasets\cite{bellamy2022batched}. The researchers used the LD50 dataset from PTDC with SMILES encodings\cite{wu2018quantitative,chen2021algebraic}, which were transformed into ECFP fingerprints\cite{rogers2010extended}, creating a 2048-dimensional feature space. Two surrogate models were examined: Gaussian Processes (GP) and Random Forests (RF). The results showed that in this high-dimensional space, Random Forests with uncertainties evaluated as variance between different trees achieved expected performance across all acquisition functions, significantly outperforming random selection. In contrast, Gaussian Processes failed to perform well, likely due to the challenges posed by the high-dimensional space ($2\textasciicircum{}{2048}$ possible combinations) and the relatively small dataset of only 7,000 data points. The researchers concluded that machine learning models, particularly Random Forests with various acquisition functions, perform well when dealing with high-dimensional molecular fingerprint spaces, demonstrating their potential for drug discovery applications with certain selection biases.
 \subsection*{Project 18: Investigation of Multi-Objective Bayesian Optimization of QM9 Dataset}


 \subsection*{Project 19: Quantum Bayesian Optimization for Automatic Chemical Design}


 \subsection*{\href{https://youtu.be/Qbvq7uolQr8}{Project 20: Closed loop optimization of hydrogel formulations using dynamic light scattering}}

This research project proposes a self-driving lab approach to optimize hydrogel formulations using BO and automated characterization techniques. The study aims to address the challenge of creating hydrogels with specific properties for applications such as cell culture, tissue engineering, drug delivery, and agriculture. The proposed workflow utilizes an open-source liquid handling robot to mix hydrogel formulations in a 96-well plate, followed by cross-linking using various methods (LED array, heating module, or time). The key innovation is the use of Dynamic Light Scattering (DLS)\cite{stetefeld2016dynamic} for automated characterization of gelation and viscoelastic properties, which can measure gel stiffness up to 10 kilopascals. This DLS method is implemented using a plate reader specifically designed for 96-well plates, enabling high-throughput analysis. The resulting property data is then fed into a Bayesian Optimizer to determine the next set of formulations and processing parameters to test. This approach aims to efficiently explore the complex relationship between hydrogel formulation parameters and their resulting properties, potentially accelerating the development of custom hydrogels for specific applications in biological research and improving data availability and reliability in the field.
 \subsection*{\href{https://www.youtube.com/watch?v=uYXAe3sRUSo}{Project 21: Benchmarking Molecular Descriptors with Actively Identified Subsets (MolDAIS)}}

This research presents a novel approach called MOLDES (Molecular Descriptors with Actively Identified Subspaces) for molecular property optimization. The method addresses the challenge of optimizing molecules in high-dimensional spaces by using molecular descriptors - sets of rotationally and translationally invariant calculations performed on molecular graphs - coupled with active subspace identification. MOLDES employs a sparse axis-aligned subspace Gaussian Process prior, which actively learns an encoding while performing Bayesian optimization. Recent works\cite{sorourifar_accelerating_2024,maus_local_2023} are increasingly turning towards active encoding of molecular feature spaces. The researchers evaluated MOLDES on three case studies: experimental lipophilicity (4,200 compounds), log P optimization benchmark (250,000 molecules), and power conversion efficiency from the Harvard Clean Energy Project (30,000 compounds). In all cases, MOLDES demonstrated superior performance compared to other optimizers, particularly in larger datasets. For the log P optimization, MOLDES consistently found the optimal molecule within 100 iterations. The method also showed strong performance in constrained optimization problems, often achieving the best-case scenario and maintaining a favorable worst-case scenario compared to other methods. Overall, MOLDES proved efficient in identifying high-performing molecules in low-data regimes, offering a promising approach for molecular property optimization tasks.
 \subsection*{\href{https://www.youtube.com/watch?v=I179UR8P054}{Project 22: Chemical Similarity-Informed Earth Mover’s Distance Kernel Bayesian Optimization for Predicting the Properties of Molecules and Molecular Mixtures}}

This research project focuses on developing chemical similarity-informed distance functions and kernels for explainable Bayesian optimization, specifically targeting the prediction of properties for molecular mixtures. The researchers propose a novel approach that bypasses the need for embedding vectors by directly providing pairwise distances between data points in the kernel function of a Gaussian Process (GP) model\cite{moss_gaussian_2020}. The project introduces the Earth Mover's Distance (EMD) kernel\cite{hargreaves_earth_2020} into the GP framework to calculate pairwise distances between mixtures based on individual component distances. This method was tested for predicting yields of binary reactant mixtures, demonstrating high chemical resolution in mixture analysis. The results show that the EMD kernel achieves accurate yield predictions with narrow distributions for both high and low-yield cases, indicating improved performance in distinguishing between different mixture compositions. By incorporating smooth distance metrics, the researchers successfully extended Bayesian optimization techniques from pure components to molecular mixtures, potentially enhancing the efficiency and interpretability of materials property prediction in complex chemical systems.
 \subsection*{Project 23: Reliable Surrogate Models of Noisy Data}


 \subsection*{\href{https://twitter.com/SodeAndy/status/1773474538631651769}{Project 24: ScattBO Benchmark - Bayesian optimisation for materials discovery}}

This project presents ScattBO, a Python-based benchmark that simulates a self-driving laboratory (SDL) for materials discovery. A self-driving laboratory is an autonomous platform that conducts machine learning-selected experiments to achieve a user-defined objective, such as synthesizing a specific material\cite{szymanski_autonomous_2023}. The benchmark addresses the challenge that such SDLs can be expensive to run, making intelligent experimental planning essential, while only a few people have access to real SDLs for materials discovery. ScattBO provides an in silico simulation of an SDL where, based on synthesis parameters, the benchmark 'synthesizes' a structure, calculates the scattering pattern\cite{johansen_gpu-accelerated_2024}, and compares it to the target structure's scattering pattern. The benchmark acknowledges that scattering data may not be sufficient to conclusively validate that the target material has been synthesized\cite{leeman_challenges_2024}, but can include other types of data as long as they can be simulated. This makes it currently challenging to benchmark Bayesian optimization algorithms for experimental planning tasks in SDLs, and ScattBO fills this gap by providing an accessible simulation environment.
 \subsection*{\href{https://www.youtube.com/watch?v=nVtTYXxG7i4}{Project 25: Bayesian Optimized De Novo Drug Design for Selective Kinase Targeting }}

This project focused on incorporating Bayesian optimization to guide de novo drug design, specifically targeting growth factor receptors for cancer therapeutics. The team built upon the DOCKSTRING paper, Python library, and dataset\cite{garcia_dockstring_2022}, using a Gaussian process with a Matérn kernel on Morgan fingerprint representations. They employed a graph genetic algorithm to generate SMILES strings guided by the Bayesian optimization output. The researchers explored both selective and promiscuous binding scenarios. For selective binding, they optimized for binding to FGFR1 while penalizing overbinding to other growth factor receptors relative to their median. For promiscuous binding, they maximized the maximum binding affinity across multiple receptors. They found that a sigmoidal penalty function was more effective than simple absolute differences when optimizing against multiple proteins. The team also incorporated a drug-likeness measure (QED)\cite{bickerton_quantifying_2012} as a penalty in the optimization process, though its effect was limited. Due to time and resource constraints, the project was unable to extensively explore the chemical space or use more accurate binding affinity calculations beyond docking. The authors suggest that future work could incorporate known unknowns through an evasion process, further optimize selective binding, and compare different molecular representations.
 \subsection*{\href{https://www.youtube.com/watch?v=wK266A0TvZ4}{Project 26: Multiple-Context Bayesian Optimization}}

This project focused on exploring multiple context Bayesian optimization using the BAYO code, a Bayesian backend package built on BoTorch with additional features. The primary aim was to investigate transfer learning capabilities by incorporating data from existing campaigns into new optimization tasks.The researchers examined both analytical functions (like the Hartmann 3D function) and real-world data (direct arylation reaction dataset) to assess the effectiveness of transfer learning in Bayesian optimization. They introduced noise, scaling, shifting, and negation to the analytical functions to simulate realistic scenarios. The results demonstrated that using a small percentage (1-25\%) of existing data significantly improved optimization performance compared to the baseline without transfer learning. Interestingly, they found that using larger amounts of data (50-100\%) did not necessarily lead to better results and in some cases performed worse than the baseline. The team also explored clustering the existing data and using cluster centroids as source data, which proved effective. For the real-world direct arylation dataset, they analyzed correlations between different reaction temperatures and observed that small amounts of transfer data (1-10\%) substantially improved the Bayesian optimization campaign. Overall, the project highlighted the potential of transfer learning in Bayesian optimization while emphasizing that careful selection of the amount and type of transfer data is crucial for optimal performance.
 \subsection*{\href{https://www.youtube.com/watch?v=4gPTMaarQt0}{Project 27: How does initial warm-up data influence Bayesian optimization in low-data experimental settings?}}

This research project investigated the influence of warm-up sampling methods and dataset sizes on property optimization in low data regimes, specifically focusing on molecular property prediction. The team used the QM9 dataset\cite{ramakrishnan_quantum_2014} and selected band gap as the optimization target. They compared two chemically-inspired sampling methods for the warm-up dataset: Morgan fingerprints and MolFormer language model fingerprints. The researchers also referenced the GDB-17 chemical universe database\cite{ruddigkeit_enumeration_2012} in their background work. The researchers performed dimensionality reduction on the fingerprints using PCA, projecting them into a 2D space for sampling. They conducted experiments to analyze how the warm-up dataset size affects optimization results. The most significant finding was the comparison between Morgan fingerprints and MolFormer fingerprints at a constant data regime of 50 data points. The results showed that MolFormer fingerprints substantially outperformed Morgan fingerprints, suggesting that pre-trained models on large chemical spaces can potentially improve model optimization rates. This study aims to initiate broader discussions on how dataset sizes and sampling methodologies impact final optimization tasks in molecular property prediction.
 \subsection*{\href{https://www.youtube.com/watch?v=hvODyYejxuc}{Project 28: The Impact of Dataset Size on Bayesian Optimization, Insights from the QM9 Dataset}}

No summary available.
 \subsection*{Project 29: A Bayesian Approach to Predict Solubility Parameters}


 \subsection*{\href{https://www.youtube.com/watch?v=78bKXIIB_GA}{Project 30: Active learning for voltammetry waveform design}}

This research project focused on using Bayesian Optimization to optimize voltammetry waveforms for sensor performance testing. The researchers employed the scikit-optimize package to implement a human-in-the-loop optimization process, allowing for pauses between optimization iterations to conduct laboratory experiments.The study encoded a four-step pulse waveform into a continuous parameter space for the optimization model. The process began with six randomly generated initial waveforms, which were tested in the lab to obtain performance metrics. These results were used to initialize a Gaussian process model. The optimization then proceeded in batches, with the model suggesting the next best waveforms to test based on previous results. The researchers conducted four total batches, updating the model after each laboratory testing phase. Progress plots were generated to visualize the improvement in sensor error across batches, demonstrating the effectiveness of the approach in finding optimal waveform parameters. The study also explored the interactions between model parameters and the performance metric across multiple dimensions, providing insights into the complex relationship between waveform characteristics and sensor performance.
 \subsection*{\href{https://www.youtube.com/watch?v=X5PDvdXoBdA}{Project 31: A tutorial on ask/tell mode for Ax}}

This project focuses on utilizing the open-source package AXS for Bayesian optimization in an asynchronous, human-in-the-loop experimental setting, particularly applied to voltammetry. The researchers developed custom figures and explanations to elucidate the complex interactions within the AXS package, emphasizing its user-friendly nature while acknowledging its potential complexity.The study explores various aspects of the optimization process, including strategy selection, surrogate model choice, and search space definition. It demonstrates two primary approaches: a "cold start" scenario, where optimization begins with no prior data, and a "warm start" scenario for situations with existing experimental data. For the cold start, the researchers showcase the use of sequence generators like Sobol sequences and random generators to initialize the optimization campaign. The project also highlights the package's ability to handle real-world experimental constraints, such as marking broken electrodes to exclude them from future suggestions. In the warm start scenario, the researchers detail the process of translating existing experimental workflows into the AXS framework, emphasizing the package's automatic tracking of experiments, results, and parameters. The study also notes the inclusion of visualization tools within AXS, though these are not described in detail.
 \subsection*{\href{https://www.youtube.com/watch?v=fxpDX7Wmdc0}{Project 32: Efficient Protein Mutagenisis using Bayesian Optimization}}

This research project explored the application of Bayesian optimization to protein mutagenesis, specifically aiming to optimize protein binding affinity to fentanyl for potential use in biosensors. The team utilized a pre-trained BERT language model, published by Andrew E. Bial, which predicts ligand binding to proteins based on amino acid sequences and SMILES strings. The methodology involved using Bayesian optimization to iteratively select positions and amino acids for mutation, starting with a protein that already had affinity to fentanyl, published by Lisa Mu Banks. The researchers focused on residues within 5 angstroms of the ligand as potential mutation sites.The team compared their Bayesian optimization approach to a baseline using random mutations. They analyzed the resulting mutations using PyRosetta's mutagenesis tool to identify potential new positive interactions and clashes that could alter protein conformation. The structural changes were further investigated using AlphaFold. While the project demonstrated the potential of Bayesian optimization for protein engineering, it was limited by its short two-day duration, suggesting room for further improvements and more extensive analysis.
 \subsection*{\href{https://www.youtube.com/watch?v=mY6Empkb8L4}{Project 33: Bayesian Optimization for Hyperspectral Co-heritability Search}}

This research project focuses on applying Bayesian Optimization (BO) to the field of crop genetics, specifically for selecting optimal proxy data to train genomic prediction models. The goal is to identify resilient crops that can withstand factors like pests and climate change. The study uses coheritability as a measure to determine the best proxy traits for desired traits, treating it as a black-box optimization problem.The researchers compared various BO methods using Gaussian Processes (GPs) against random search for four different desired traits. Surprisingly, they found that all BO methods performed similarly to random search. However, a pre-trained probabilistic model with a mixed multitask kernel outperformed standard BO approaches, identifying better proxy data more quickly. The study concludes that Gaussian Processes may not be the optimal choice for this particular problem, challenging the common use of Matérn kernels and GPs as the default in Bayesian Optimization. The researchers also criticize the widespread use of these methods without proper justification. As part of their contribution, the team has released their dataset for further research in this area.
 \subsection*{Project 34: Streamlining Material Discovery - Bayesian Optimization in Thermal Fluid Mixtures}


 \subsection*{\href{https://x.com/Ryan__Rhys/status/1820723528469262419}{Project 35: Tutorial for GAUCHE - A Library for Gaussian Processes in Chemistry}}

This research project focuses on implementing input warping for Bayesian Optimization within the Gauche library\cite{griffiths_gauche_2024}, which was previously developed by the team and published at NeurIPS 2023. The primary innovation of Gauche is the introduction of Gaussian process (GP) kernels that enable modeling of discrete entities such as SMILES strings, graphs, and bit vectors, which are common representations in molecular sciences. The motivation behind using Gaussian processes for Bayesian Optimization is their suitability for automated tasks where fine-tuning for each problem is not feasible. GPs offer a good balance between performance and simplicity, with few trainable hyperparameters that can reliably converge on each iteration of the Bayesian Optimization loop. This makes them particularly attractive as surrogate models compared to more complex alternatives like deep neural networks, which might require careful monitoring during training at each iteration. The Gauche library extends the applicability of GPs to discrete input spaces, allowing for Bayesian Optimization over molecular representations. The project team has developed a range of tutorials and applications, including molecular property prediction, protein fitness prediction, and sparse GP regression, all available in the Gauche GitHub repository.
 \subsection*{\href{https://youtu.be/CXweDiS_wbI}{Project 36: Scalable Nonmyopic Bayesian Optimization in Dynamic Cost Settings}}

This research project focuses on scalable Bayesian optimization in dynamic settings, addressing limitations of previous approaches that rely on myopic acquisition functions and assume fixed cost structures. The researchers introduce a novel method using non-myopic acquisition functions\cite{jiang_efficient_2020} that incorporate a look-ahead mechanism and dynamic cost functions. The project evaluates the proposed algorithm, named HBE, through two main experimental setups. First, they use synthetic functions across 14 different environments with varying dimensions to test scalability. Second, they apply the method to a real-world protein sequence design problem, aiming to maximize a protein score. The researchers compare their HBE algorithm against six other acquisition functions, including state-of-the-art methods. To enhance practicality, they integrate automatic hyperparameter tuning to reduce the number of optimization parameters. While specific results are not provided in the given context, the approach aims to overcome suboptimal resource allocation in dynamic cost experiments and improve upon existing Bayesian optimization techniques.
 \subsection*{\href{https://www.youtube.com/watch?v=kwXHoWV8g1E}{Project 37: The Effects of Post-Modelling Performance Metric Computation on the Efficiency of Bayesian Optimizers}}

This research project explored modifications to the Bayesian Optimization algorithm to enhance its efficiency in solving real-world problems. The key innovation was changing the order of objective calculation and model fitting. Instead of calculating all objectives first and then modeling each objective, the proposed method only models the essential observations (e.g., yield) and calculates the objectives after the modeling process. This approach was tested using a simulation of a real-world system based on kinetics discovered by Hornel, employing a plug flow reactor. The optimization aimed to maximize both space-time yield and E factor for the predicted yield.The results demonstrated significant time savings by computing objectives after modeling, primarily because only a single model for yield needed to be fitted instead of separate models for space-time yield and E factor. While computational performance improved, there was negligible impact on convergence speed. Interestingly, the research suggests that this new approach might potentially improve the efficiency of selection criteria by allowing the fitting of a prior distribution biased towards regions with more potential based on the objective function's structure. In summary, the study found that fitting objectives after modeling can lead to substantial increases in computational performance, with the possibility of also improving hypervolume convergence efficiency.
 \subsection*{\href{https://www.youtube.com/watch?v=KKxlqo15wVY}{Project 38: Bayesian methods in symbolic regression}}

This research project focused on enhancing symbolic regression methods for physical science data sets using Bayesian optimization techniques. The researchers explored two main approaches to improve the performance of symbolic regression algorithms.First, they investigated the impact of assigning varying importance to individual data points under data-limited conditions. Using the Optuna framework, they implemented a tuning process for data point importance, which resulted in improved predictive performance on a benchmark set compared to treating all points equally. This approach could be particularly useful when dealing with experimental data of uncertain quality. Second, they integrated prior knowledge from Wikipedia's known mathematical equations into the loss function, combining it with the Bayesian Information Criterion to statistically favor established symbolic structures. This method was implemented as a fitness function in the Symbolic Regression Julia library. While the researchers faced challenges in evaluating results due to numerical instability, both approaches showed promise in improving symbolic regression performance by incorporating uncertainty and prior information. The team suggests that future work should adhere more closely to the original paper's methodology, using the Bayesian information prior differences between equations as a probability metric for accepting mutational operations in the symbolic regression process.
 \subsection*{\href{https://www.youtube.com/watch?v=iog-07Ekp9g}{Project 39: Divide and Conquer - Local Gaussian Processes to design Covalent Organic Frameworks for Methane Deliverable Capacity}}

This research project focuses on improving Bayesian Optimization (BO) for high-dimensional, large-scale datasets, specifically applied to the design of Covalent Organic Frameworks (COFs) for methane storage\cite{deshwal_bayesian_2021}. The researchers developed a novel approach combining unsupervised clustering with local Gaussian Process (GP) models to enhance BO efficiency in the high data regime. The method begins by using K-means clustering to partition the dataset into distinct clusters. A portion of data from each cluster is then sampled to train separate local GP models. An epsilon-greedy algorithm is employed to determine which GP to train next. The researchers applied this approach to a COF dataset containing over 70,000 2D and 3D structures, assembled in silico from 666 organic linkers and four synthetic routes. The objective was to maximize methane storage performance, measured as deliverable capacity. The results demonstrated that their divide-and-conquer approach with local GP surrogates significantly outperformed a single GP model. While the single GP model reached a maximum deliverable capacity of 2.74 in 60 iterations, the proposed method surpassed this maximum within 5-10 iterations. Both methods retrained the GP model after 20 iterations and used 5\% of the data for initial surrogate model training. This research highlights the potential of using local GP surrogates in combination with unsupervised clustering to perform more efficient Bayesian optimization in high-dimensional, large-scale datasets.
 \subsection*{\href{https://x.com/Bozhao95501764/status/1777029207857451508}{Project 40: Optimizing Chemical Reaction Conditions with Multi-Agent Systems Using LLM and BO}}

This research project focuses on optimizing Suzuki reaction conditions using Bayesian optimization enhanced by a multi-agent large language model (LLM) system. The target dataset consists of Suzuki reactions, with the goal of optimizing reaction ligands, bases, and solvents. The researchers observed that while the target dataset contains unique combinations of conditions, many of the ligands are similar to those in existing reaction data points, suggesting potential for knowledge transfer.The developed system employs a multi-agent LLM approach comprising three domain expert agents and a reaction optimization agent. The process involves four steps: (1) the reaction optimization agent assigns tags to domain agents, (2) gathers feedback from them, (3) uses this feedback to warm-start initial data point generation and evaluation, and (4) acquires new data points based on existing data. In demo experiments, the multi-agent LLM system demonstrated improved performance, reaching target reaction yields with fewer iterations compared to traditional methods. Additionally, the multi-agent approach showed an ability to avoid local maxima traps, which is particularly valuable given the challenging nature of reaction optimization due to data sparsity. This research highlights the potential of integrating LLM-based multi-agent systems with Bayesian optimization techniques to enhance reaction condition optimization in chemistry.
 \subsection*{\href{https://x.com/6ojaHa/status/1773734082637095155}{Project 41: Retrieval Augmented Bayesian Optimization}}

This research project, called RAMBO, combines retrieval-augmented generation with Bayesian optimization to enhance the initial point selection for new optimization tasks. The system leverages existing data from literature or internal databases to identify optimal starting points for Bayesian optimization processes, particularly in the context of chemical reactions.The RAMBO pipeline begins by describing the design space of a reaction to be optimized and determining which reaction parameters should be explored first. It then queries available data to extract the most relevant data points and assembles initial suggestions for the user. The system was demonstrated using a Suzuki-Miura coupling reaction as an example, which involves an extensive combinatorial space of different parameters. RAMBO can describe the design space and limit it to available compounds, providing starting parameters to optimize the reaction. The project includes a demo interface where users can input their reaction of interest, and the React system in the backend queries and extracts relevant data to form a reply containing necessary conditions to initiate the Bayesian optimization process. Users can also explore the reasoning behind the system's suggestions by analyzing the data and reactions used to generate the final answer.
 \subsection*{Project 42: Project 42 Mqs_bodoe}


 \subsection*{Project 43: Bayesian Optimization Awesome List}


 \subsection*{\href{https://www.youtube.com/watch?v=c84Sd2IwMAQ&ab_channel=GaryTom}{Project 44: Rank-based Bayesian Optimization}}

This research project focused on using ranking models as surrogates in Bayesian optimization for materials discovery, specifically comparing ranking-based and conventional mean squared error (MSE) loss approaches. The key motivation was that in experimental campaigns, finding the best molecule is more important than accurately predicting absolute property values, inspired by work on molecular pool-based active learning\cite{graff_accelerating_2021}. The study employed a pairwise ranking loss (margin ranking loss) and a simple fully-connected multi-layer perceptron with three hidden layers and 100 nodes as the model architecture. Experiments were conducted on multiple datasets, including a solubility dataset (Delaney) and two datasets from Ali et al. with varying roughness\cite{aldeghi_roughness_2022}. Results showed that the ranking loss model consistently outperformed the MSE-based model and random baseline in Bayesian optimization, acquiring more top-performing candidates within fewer evaluations. Interestingly, all models performed better on the smoother dataset, contrary to expectations. The study also found that model performance did not always correlate with its effectiveness as a surrogate in Bayesian optimization, as evidenced by the MSE model performing worse than the random baseline on the rougher dataset. The research highlights the potential of ranking-based models in overcoming overfitting issues common in Bayesian optimization with limited data points, particularly in materials discovery applications.
 \subsection*{\href{https://www.youtube.com/watch?v=wfSyIudptfc}{Project 45: Bayesian Optimization for generality}}

This research introduces a novel approach to Bayesian Optimization (BO) focused on finding general parameters that perform well across multiple related tasks, a common challenge in fields like chemistry where optimal reaction conditions for various substrates are sought. The researchers formulate this problem within the BO framework, aiming to maximize a general response from a set of multiple related objective functions.The proposed algorithm operates in two steps: first selecting a value in the optimization domain X, then choosing an individual objective function to optimize for that value. This approach aims to find the most general values in X without exhaustively evaluating every individual surface, thus minimizing evaluation costs. To support further research in this area, the team adapted popular test surfaces (Dixon-Price, Branin, and Beale) to be compatible with their generality problem and made these benchmark problems available on Hugging Face. This contribution allows other researchers to test and develop new algorithms for generalized Bayesian Optimization, potentially advancing the field and its applications in natural sciences.

\subsection*{\href{https://www.youtube.com/watch?v=Z7YiruHv3eE}{Project 1: Multi-objective Benchmarking of Dragonfly against BoTorch}}

This research project compared the performance of two BO packages, Dragonfly\cite{JMLRdragonfly} and BoTorch\cite{balandat2020botorch}, in a multi-objective optimization context. The researchers optimized the Branin-Currin test function with added noise, conducting 20 trials of BO over different random seeds. Each trial used a batch size of 1 and consisted of 20 iterations, evaluating 20 candidates per iteration. The results showed that BoTorch's acquisition functions, particularly the q-noisy expected hypervolume improvement (qNEHVI)\cite{daulton2021parallel}, outperformed Dragonfly's acquisition function. The qNEHVI acquisition function achieved the lowest log hypervolume difference on average across the trials, while Dragonfly's acquisition function struggled to surpass a randomly generated set of candidates. Visualization of the average Pareto front produced by different acquisition functions further demonstrated BoTorch's superiority, with its acquisition functions covering a larger portion of the Pareto front compared to Dragonfly. The researchers concluded that BoTorch outperformed Dragonfly within the scope of their study, but noted the need for further investigation using different batch sizes, input and output constraints, and test functions simulating chemical reactions to apply the findings to material design problems.
 \subsection*{\href{https://www.youtube.com/watch?v=pegJumJEOsE}{Project 2: Long-run Behaviour of Multi-fidelity Bayesian Optimisation}}

This research project investigates the long-term performance of multifidelity Bayesian optimization (MFBO) methods\cite{poloczek2017multi} compared to single-fidelity Bayesian optimization (SFBO) methods. The study focuses on scenarios where two fidelities are available: an accurate but expensive target fidelity that fully represents the objective, and a less accurate but cheaper low fidelity. The researchers observed that while MFBO methods initially outperform SFBO up to a budget of approximately 20, they begin to underperform beyond this point. This behavior is particularly evident with linear kernel MFBO methods, where the performance gap becomes more pronounced after a cost of around 60.The project aims to evaluate the factors contributing to the long-term issues of MFBO methods. Specifically, the researchers plan to investigate the impact of kernel choice, frequency of low fidelity queries, and acquisition function selection on MFBO performance. By analyzing these aspects, the study seeks to identify potential improvements for MFBO methods to enhance their applicability in real-life optimization tasks. The ultimate goal is to develop MFBO approaches that can maintain their performance advantage over SFBO methods in the long run, leveraging the cost-effectiveness of low fidelity queries while achieving superior optimization results.
 \subsection*{\href{https://www.youtube.com/watch?v=WkGfShRSYW4}{Project 3: Take Your Time - Improving Optimization Performance Through Greater Investment in ACQF Optimizer Runtime}}

This research project explores an enhancement to standard BO campaigns by introducing a random seed approach to improve optimization outcomes. The team from the University of Utah's Material Science and Engineering department investigated the variability in BO campaigns when starting from the same initial data points. The key innovation in this work is a custom approach to calculating the acquisition function. Instead of computing it once per iteration, as is standard in most BO methods\cite{shahriari2015taking, snoek2012practical}, the researchers calculate the acquisition function multiple times using different random seeds. They then select the highest-performing result at each iteration. This method, termed "random retries Optimizer", showed significant improvements in optimization performance, particularly in avoiding local optima. The researchers demonstrated that this approach consistently achieves near-optimal performance among the possible campaigns stemming from the initial data points. While this method requires additional computational time for the multiple acquisition function calculations, the performance gains appear to outweigh the increased computational cost. The team is continuing to investigate questions such as the trade-off between additional compute time and performance improvements, as well as how the method performs across different types of optimization problems, including simpler ones compared to the challenging example presented.
 \subsection*{\href{https://www.youtube.com/watch?v=Qr2cz5lxM64&ab_channel=ArifinSan}{Project 4: SimpleGPT-BO, Simplified GPT-Powered Bayesian Optimization}}

No summary available.
 \subsection*{\href{https://www.youtube.com/watch?v=znXZhSqFtHg}{Project 5: Comparing Bayesian Optimization Methods Across Multiple Hyperparameters Against Simulated "Human" Decision-making}}

This research project investigated the effectiveness of BO methods that mimic human experimentalist approaches in materials science\cite{muckley2023interpretable}, comparing them to traditional BO techniques and random search baselines\cite{snoek2012practical,bergstra2012random}. The study focused on three key axes: model complexity, number of features, and acquisition function. The researchers found that a simplified "human experimentalist" approach, using automated feature engineering with a linear model, three features, and an exploitative acquisition function, was competitive with more complex random forest models in terms of enhancement factor and acceleration factor. Both the simplified approach and random forest models outperformed random search. Interestingly, for linear models, increasing complexity through additional features or more exploratory acquisition functions led to decreased performance. The main conclusion was that low-dimensional, interpretable models were comparable to traditional BO methods for the datasets and hyperparameter regimes studied. The researchers suggest future work could involve comparing their results to actual human performance and improving their methodology using theoretical approaches.
 \subsection*{\href{https://www.youtube.com/watch?v=RgEbcWIBDn8}{Project 6: Multi-Objective Bayesian Optimization for Transparent Electromagnetic Interference Shielding with Thin-Film Structures}}

This research project focuses on applying multi-objective BO to design transparent electromagnetic interference (EMI) shielding using thin film structures\cite{li2022bayesian}. The primary objectives are to maximize both transmittance and shielding effectiveness simultaneously. The optimization problem involves selecting materials and thicknesses for each layer of the thin film structure, with a search space comprising 12 material choices (Ag, Al, Al$_2$O$_3$, Cr, Ni, Pd, Si$_3$N$_4$, SiO$_2$, Ti, TiN, TiO$_2$, W) and thickness ranges from 5 to 20 nm. The researchers implemented multi-objective BO using random scalarization for the acquisition function via BayesO package\cite{kim2023bayeso}. Gaussian process regression and expected improvement were employed as part of the BO framework. The results demonstrate that the multi-objective BO approach effectively identified the Pareto frontier for the two objectives of transmittance and shielding effectiveness. This suggests that the method is successful in finding optimal trade-offs between transparency and EMI shielding performance for thin film structures, which could have applications in areas such as spacecraft windows where both properties are crucial.
 \subsection*{\href{https://www.youtube.com/watch?v=kIRxGdwmLSY}{Project 7: BayBE One More Time - Exploring Corrosion Inhibitors for Materials Design}}

This research project explored the use of BO for efficiently identifying effective corrosion inhibitors in materials design. The researchers utilized the BO implementation from the BayBE package\cite{fitzner2022baybe} to evaluate its performance on multiple experimental datasets\cite{galvao2022cordata} involving different alloys, primarily aluminum-based. The study found that the choice of molecular descriptor encoding significantly impacted the optimization performance, especially for larger datasets like A1000 and A2024. The SMILES-based approach with Mordred encoding\cite{moriwaki2018mordred} outperformed the other methods for these datasets, while random sampling underperformed. Interestingly, for the A775 dataset, which had a sparse distribution of reported efficiencies and fewer data points, this trend was not observed. The researchers also explored transfer learning capabilities, using information from the A2024 alloy dataset to inform the optimization process for the A1000 dataset. This transfer learning approach outperformed the uninformed method after only 15 iterations, demonstrating its potential for accelerating the discovery of effective corrosion inhibitors in experimental settings.
 \subsection*{\href{https://www.youtube.com/watch?v=5f_UwsfYrc8}{Project 8: BO for Drug Discovery-What is the role of molecular representation?}}

This research project investigated the impact of molecular featurization methods on BO performance for guiding molecular experiments. The team explored alternatives to the commonly used connectivity-based fingerprints, aiming to determine which featurization techniques yield the best results in BO for molecular discovery.The study found that without modifying the features, MACCS\cite{durant2002reoptimization} and RDKit\cite{landrum2013rdkit} featurization methods outperformed the default options presented in previous publications. To address the high dimensionality of molecular features, the researchers employed specialized Gaussian processes and explored random forest surrogates. However, the random forest approach did not improve performance and required more computational resources. Principal Component Analysis (PCA) was successfully used to enable Gaussian processes to handle large molecular featurizations. Notably, physicochemical featurizations like RDKit and Mordred\cite{moriwaki2018mordred} outperformed the previous benchmark winner, MolDQN\cite{zhou2019optimization}. The team's findings were consistent with benchmarks in the BO library but suggested that RDKit could produce even better results than Mordred. Overall, the research highlights the importance of careful featurization selection in BO for molecular discovery, with RDKit emerging as a particularly effective option.
 \subsection*{\href{https://youtu.be/l0aVZDMwIMU}{Project 9: Optimal MOF Selection for CO$_2$ capture using Thompson sampling}}

This research project focuses on optimizing the selection of Metal-Organic Frameworks (MOFs) for carbon capture applications using BO, specifically Thompson Sampling. MOFs are nanoporous materials with high potential for carbon capture\cite{furukawa2013chemistry, heo2020metal}, but their synthesis and characterization are expensive and time-consuming\cite{desantis2017techno}. The goal was to develop an efficient method for identifying high-performing MOF candidates using a small dataset. The team employed a Gaussian Process model trained on the CRAFTED dataset\cite{oliveira2023crafted}, using both RACs features and geometric features of MOFs, with CO$_2$ uptake as the output. Thompson Sampling was used as the acquisition function to select the next best sample based on the posterior distribution, inherently performing a Bayesian update. The results showed that Thompson Sampling was twice as efficient as random sampling in identifying high-performing MOF candidates. This approach is notable for being the first application of Thompson Sampling to MOF candidate selection, and it requires no hyperparameter tuning, making it easily transferable and cost-efficient. The potential impact of this method is significant, as it could accelerate MOF design for direct air capture applications, with a goal of absorbing 30 million tons of CO$_2$, and could also be applied to post-combustion capture scenarios.
 \subsection*{\href{https://www.youtube.com/watch?v=4lFEUixwkE8}{Project 10: Navigating the black box of zeolite synthesis with Bayesian Optimization}}

This research project explores the application of BO to zeolite synthesis, a complex process with significant industrial importance. Zeolites are crystalline materials composed of interconnected silicate and aluminate tetrahedra, widely used as absorbents and catalysts\cite{dusselier2018small}. The synthesis of zeolites involves multiple parameters, including silicon and aluminum sources, organic molecules, water, temperature, and time. The process aims to achieve specific properties such as high crystallinity, large external surface area, and particular silicon-to-aluminum ratios, while also considering economic factors like synthesis temperature, duration, and ingredient concentrations\cite{mallette2024current}. The project presents a GitHub repository containing an introductory text on zeolites and their synthesis, along with a Jupyter notebook demonstrating BO applications. The notebook is divided into two main sections: the first optimizes an analytical dummy function using zeolite synthesis parameters, exploring various aspects of BO including continuous, categorical, and mixed variable types, parameter constraints, and single and multiple objectives. The second section applies BO to propose new experiments based on existing literature data. This approach aims to accelerate the traditionally time-consuming trial-and-error process of zeolite synthesis optimization, potentially reducing costs and improving efficiency in industrial applications.
 \subsection*{\href{https://www.youtube.com/watch?v=HASa3tFLZoI}{Project 11: BlendDS - An intuitive specification of the design space for blends of components}}

This project focuses on developing an interface for BO that bridges the gap between domain experts (i.e. chemists and materials scientists) and machine learning algorithms. The system allows scientists to specify experimental parameters and constraints in natural language, which is then translated into a structured dictionary format by a large language model. This dictionary is subsequently converted into a Python object that can be used for design of experiments and optimization tasks.The key feature of this interface is its ability to generate diverse trial designs for efficient sampling of the experimental space. It incorporates dimensionality reduction techniques to visualize the design space, enabling users to select the most diverse and informative trials. This approach aims to maximize the information gained from each experiment, potentially reducing the number of trials needed to reach optimal results. The project is open-source, inviting contributions and integration with existing BO frameworks.
 \subsection*{\href{https://www.youtube.com/watch?v=jSPGCgH31Hc}{Project 12: Robust GPs for Sustainable Concrete via Bayesian Optimization}}

This project introduces Robust Gaussian Processes (Robust GPs)\cite{altamirano2023robust} to the concrete sciences experiments with Bayesian optimization campaigns. The research addresses the common issue of outliers in concrete science experiments by applying Robust GPs to account for these anomalies. The project provides a tutorial and accompanying code in a GitHub repository, demonstrating how to implement Robust GPs in this specific context. The study utilizes a dataset provided by Meta\cite{ament2310sustainable}, which includes input variables and a target variable representing concrete strength. Initial data analysis reveals potential outliers in the strength variable, with values between 2 and 2,000. The researchers then apply Robust GP to this dataset, achieving good predictive performance. The project culminates in demonstrating how to obtain Bayesian recommendations using the Robust GP as a surrogate model, providing a practical approach for optimizing concrete mixtures while accounting for experimental outliers.
 \subsection*{\href{https://www.youtube.com/watch?v=Cyyj9ySybZE}{Project 13: Interpretability of Bayesian Optimisation Campaigns}}

This research project focused on developing novel methods for interpreting Bayesian Optimization campaigns by incorporating the temporal component often neglected in end-of-campaign analyses. The study utilized data from a self-driving lab experiment aimed at optimizing the conductivity of a coating. The researchers recreated the Gaussian Process (GP) model employed in the original experiment, training it on subsets of data to simulate mid-campaign conditions. Two primary methods were investigated: First, cross-sections of the GP model were taken at various stages of the campaign, holding all but one feature constant. This approach revealed that closer nozzle distances produced better results, aligning with prior beliefs and serving as a potential mid-campaign sense check to validate model alignment with previous experimentation. The second method involved predicting the error of the next sample based on the model trained on data collected up to that point. This analysis showed that performance improvement plateaued around the time the optimal sample was produced, suggesting its potential use as an early stopping criterion if error is monitored throughout the campaign.The research demonstrates the value of incorporating temporal analysis in Bayesian Optimization campaigns, offering insights that could enhance decision-making during experiments. The cross-sectional approach provides a means to validate model behavior against prior knowledge, while the error prediction method could inform stopping criteria, potentially improving efficiency in optimization processes. These techniques offer promising avenues for real-time interpretation and guidance in Bayesian Optimization experiments, particularly in self-driving lab contexts.
 \subsection*{Project 14: Bayesian optimization of likely negative candidates in imbalanced biological datasets}


 \subsection*{\href{https://www.youtube.com/watch?v=utnWbJsObF0}{Project 15: Adaptive Batch Sizes for Bayesian Optimization of Reaction Yield}}

This research project investigates the optimization of batch sizes in BO for chemical reaction yield optimization. The study focuses on determining the optimal batch size (Q) at each step of the BO process, considering both smaller and larger batch sizes. The researchers examine how Q impacts the cost of retraining models, such as large language models, and how it relates to additional experimentation overhead. The project findings indicate that the optimal Q depends on a trade-off between model retraining time and additional experimentation overhead. For a fixed batch size, the researchers observed two minima at Q=3 and Q=7, reflecting the relative importance of Q in different scenarios. With very large batch sizes, the retraining cost had minimal impact on the overall experimental overhead, while experimentation overhead had a significant influence. The study also explored adaptive batch sizing strategies, decreasing batch sizes when performance was good and increasing them as the surrogate model became more trustworthy. The researchers conclude that batch sizes have a crucial effect on BO efficiency in real-world settings, and adaptive batch sizes could effectively balance the trade-off between model retraining and batch sampling effectiveness.
 \subsection*{\href{https://www.youtube.com/watch?v=AbRDOdmafB8}{Project 16: BOPE-GPT, Preference Exploration with the curious AI chemist}}

This research project focused on optimizing the Fischer-Tropsch synthesis process\cite{mahmoudi2017review}, which converts syngas into biofuel hydrocarbons. The optimization problem on the dataset\cite{lozano2008single,chakkingal2022multi} involved four input variables (space time, syngas ratio, temperature, and pressure) and four output variables (carbon conversion, selectivity, methane to paraffins, and olefins). The researchers proposed a novel approach using preferential BO\cite{lin2022preference} with large language models (LLMs) to prioritize outputs based on stakeholder preferences. The methodology employed the typical BO process powered by BoTorch, using Expected Utility as the acquisition function. However, the innovation lay in using an LLM (specifically GPT-4) to perform pairwise comparisons for output prioritization. The results showed that outputs exhibited well-defined optima trade-offs and some degree of monotonicity, which was important for preferential BO. The researchers tested three objective scenarios: optimizing all outputs equally, maximizing only CO conversion, and maximizing three objectives while minimizing olefins. For the first two objectives, the LLM-based preferential BO performed similarly to traditional utility function-based methods. However, for the third, more complex objective, the LLM approach performed worse than the utility function method and only slightly better than random exploration, suggesting limitations in the LLM's ability to handle complex optimization scenarios. The project also included the development of an interactive application for visualizing BO results and running live optimizations.
 \subsection*{\href{https://www.youtube.com/watch?v=5AjwoZtjgOc}{Project 17: Comparative Analysis of Acquisition Functions in Bayesian Optimization for Drug Discovery}}

This project investigates the application of BO techniques directly on molecular fingerprints for drug discovery, focusing on comparing different surrogate models, acquisition functions and small, diverse, unbalanced, and noisy datasets\cite{bellamy2022batched}. The researchers used the LD50 dataset from PTDC with SMILES encodings\cite{wu2018quantitative,chen2021algebraic}, which were transformed into ECFP fingerprints\cite{rogers2010extended}, creating a 2048-dimensional feature space. Two surrogate models were examined: Gaussian Processes (GP) and Random Forests (RF). The results showed that in this high-dimensional space, Random Forests with uncertainties evaluated as variance between different trees achieved expected performance across all acquisition functions, significantly outperforming random selection. In contrast, Gaussian Processes failed to perform well, likely due to the challenges posed by the high-dimensional space ($2\textasciicircum{}{2048}$ possible combinations) and the relatively small dataset of only 7,000 data points. The researchers concluded that machine learning models, particularly Random Forests with various acquisition functions, perform well when dealing with high-dimensional molecular fingerprint spaces, demonstrating their potential for drug discovery applications with certain selection biases.
 \subsection*{Project 18: Investigation of Multi-Objective Bayesian Optimization of QM9 Dataset}


 \subsection*{Project 19: Quantum Bayesian Optimization for Automatic Chemical Design}


 \subsection*{\href{https://youtu.be/Qbvq7uolQr8}{Project 20: Closed loop optimization of hydrogel formulations using dynamic light scattering}}

This research project proposes a self-driving lab approach to optimize hydrogel formulations using BO and automated characterization techniques. The study aims to address the challenge of creating hydrogels with specific properties for applications such as cell culture, tissue engineering, drug delivery, and agriculture. The proposed workflow utilizes an open-source liquid handling robot to mix hydrogel formulations in a 96-well plate, followed by cross-linking using various methods (LED array, heating module, or time). The key innovation is the use of Dynamic Light Scattering (DLS)\cite{stetefeld2016dynamic} for automated characterization of gelation and viscoelastic properties, which can measure gel stiffness up to 10 kilopascals. This DLS method is implemented using a plate reader specifically designed for 96-well plates, enabling high-throughput analysis. The resulting property data is then fed into a Bayesian Optimizer to determine the next set of formulations and processing parameters to test. This approach aims to efficiently explore the complex relationship between hydrogel formulation parameters and their resulting properties, potentially accelerating the development of custom hydrogels for specific applications in biological research and improving data availability and reliability in the field.
 \subsection*{\href{https://www.youtube.com/watch?v=uYXAe3sRUSo}{Project 21: Benchmarking Molecular Descriptors with Actively Identified Subsets (MolDAIS)}}

This research presents a novel approach called MOLDES (Molecular Descriptors with Actively Identified Subspaces) for molecular property optimization. The method addresses the challenge of optimizing molecules in high-dimensional spaces by using molecular descriptors - sets of rotationally and translationally invariant calculations performed on molecular graphs - coupled with active subspace identification. MOLDES employs a sparse axis-aligned subspace Gaussian Process prior, which actively learns an encoding while performing Bayesian optimization. Recent works\cite{sorourifar_accelerating_2024,maus_local_2023} are increasingly turning towards active encoding of molecular feature spaces. The researchers evaluated MOLDES on three case studies: experimental lipophilicity (4,200 compounds), log P optimization benchmark (250,000 molecules), and power conversion efficiency from the Harvard Clean Energy Project (30,000 compounds). In all cases, MOLDES demonstrated superior performance compared to other optimizers, particularly in larger datasets. For the log P optimization, MOLDES consistently found the optimal molecule within 100 iterations. The method also showed strong performance in constrained optimization problems, often achieving the best-case scenario and maintaining a favorable worst-case scenario compared to other methods. Overall, MOLDES proved efficient in identifying high-performing molecules in low-data regimes, offering a promising approach for molecular property optimization tasks.
 \subsection*{\href{https://www.youtube.com/watch?v=I179UR8P054}{Project 22: Chemical Similarity-Informed Earth Mover’s Distance Kernel Bayesian Optimization for Predicting the Properties of Molecules and Molecular Mixtures}}

This research project focuses on developing chemical similarity-informed distance functions and kernels for explainable Bayesian optimization, specifically targeting the prediction of properties for molecular mixtures. The researchers propose a novel approach that bypasses the need for embedding vectors by directly providing pairwise distances between data points in the kernel function of a Gaussian Process (GP) model\cite{moss_gaussian_2020}. The project introduces the Earth Mover's Distance (EMD) kernel\cite{hargreaves_earth_2020} into the GP framework to calculate pairwise distances between mixtures based on individual component distances. This method was tested for predicting yields of binary reactant mixtures, demonstrating high chemical resolution in mixture analysis. The results show that the EMD kernel achieves accurate yield predictions with narrow distributions for both high and low-yield cases, indicating improved performance in distinguishing between different mixture compositions. By incorporating smooth distance metrics, the researchers successfully extended Bayesian optimization techniques from pure components to molecular mixtures, potentially enhancing the efficiency and interpretability of materials property prediction in complex chemical systems.
 \subsection*{Project 23: Reliable Surrogate Models of Noisy Data}


 \subsection*{\href{https://twitter.com/SodeAndy/status/1773474538631651769}{Project 24: ScattBO Benchmark - Bayesian optimisation for materials discovery}}

This project presents ScattBO, a Python-based benchmark that simulates a self-driving laboratory (SDL) for materials discovery. A self-driving laboratory is an autonomous platform that conducts machine learning-selected experiments to achieve a user-defined objective, such as synthesizing a specific material\cite{szymanski_autonomous_2023}. The benchmark addresses the challenge that such SDLs can be expensive to run, making intelligent experimental planning essential, while only a few people have access to real SDLs for materials discovery. ScattBO provides an in silico simulation of an SDL where, based on synthesis parameters, the benchmark 'synthesizes' a structure, calculates the scattering pattern\cite{johansen_gpu-accelerated_2024}, and compares it to the target structure's scattering pattern. The benchmark acknowledges that scattering data may not be sufficient to conclusively validate that the target material has been synthesized\cite{leeman_challenges_2024}, but can include other types of data as long as they can be simulated. This makes it currently challenging to benchmark Bayesian optimization algorithms for experimental planning tasks in SDLs, and ScattBO fills this gap by providing an accessible simulation environment.
 \subsection*{\href{https://www.youtube.com/watch?v=nVtTYXxG7i4}{Project 25: Bayesian Optimized De Novo Drug Design for Selective Kinase Targeting }}

This project focused on incorporating Bayesian optimization to guide de novo drug design, specifically targeting growth factor receptors for cancer therapeutics. The team built upon the DOCKSTRING paper, Python library, and dataset\cite{garcia_dockstring_2022}, using a Gaussian process with a Matérn kernel on Morgan fingerprint representations. They employed a graph genetic algorithm to generate SMILES strings guided by the Bayesian optimization output. The researchers explored both selective and promiscuous binding scenarios. For selective binding, they optimized for binding to FGFR1 while penalizing overbinding to other growth factor receptors relative to their median. For promiscuous binding, they maximized the maximum binding affinity across multiple receptors. They found that a sigmoidal penalty function was more effective than simple absolute differences when optimizing against multiple proteins. The team also incorporated a drug-likeness measure (QED)\cite{bickerton_quantifying_2012} as a penalty in the optimization process, though its effect was limited. Due to time and resource constraints, the project was unable to extensively explore the chemical space or use more accurate binding affinity calculations beyond docking. The authors suggest that future work could incorporate known unknowns through an evasion process, further optimize selective binding, and compare different molecular representations.
 \subsection*{\href{https://www.youtube.com/watch?v=wK266A0TvZ4}{Project 26: Multiple-Context Bayesian Optimization}}

This project focused on exploring multiple context Bayesian optimization using the BAYO code, a Bayesian backend package built on BoTorch with additional features. The primary aim was to investigate transfer learning capabilities by incorporating data from existing campaigns into new optimization tasks.The researchers examined both analytical functions (like the Hartmann 3D function) and real-world data (direct arylation reaction dataset) to assess the effectiveness of transfer learning in Bayesian optimization. They introduced noise, scaling, shifting, and negation to the analytical functions to simulate realistic scenarios. The results demonstrated that using a small percentage (1-25\%) of existing data significantly improved optimization performance compared to the baseline without transfer learning. Interestingly, they found that using larger amounts of data (50-100\%) did not necessarily lead to better results and in some cases performed worse than the baseline. The team also explored clustering the existing data and using cluster centroids as source data, which proved effective. For the real-world direct arylation dataset, they analyzed correlations between different reaction temperatures and observed that small amounts of transfer data (1-10\%) substantially improved the Bayesian optimization campaign. Overall, the project highlighted the potential of transfer learning in Bayesian optimization while emphasizing that careful selection of the amount and type of transfer data is crucial for optimal performance.
 \subsection*{\href{https://www.youtube.com/watch?v=4gPTMaarQt0}{Project 27: How does initial warm-up data influence Bayesian optimization in low-data experimental settings?}}

This research project investigated the influence of warm-up sampling methods and dataset sizes on property optimization in low data regimes, specifically focusing on molecular property prediction. The team used the QM9 dataset\cite{ramakrishnan_quantum_2014} and selected band gap as the optimization target. They compared two chemically-inspired sampling methods for the warm-up dataset: Morgan fingerprints and MolFormer language model fingerprints. The researchers also referenced the GDB-17 chemical universe database\cite{ruddigkeit_enumeration_2012} in their background work. The researchers performed dimensionality reduction on the fingerprints using PCA, projecting them into a 2D space for sampling. They conducted experiments to analyze how the warm-up dataset size affects optimization results. The most significant finding was the comparison between Morgan fingerprints and MolFormer fingerprints at a constant data regime of 50 data points. The results showed that MolFormer fingerprints substantially outperformed Morgan fingerprints, suggesting that pre-trained models on large chemical spaces can potentially improve model optimization rates. This study aims to initiate broader discussions on how dataset sizes and sampling methodologies impact final optimization tasks in molecular property prediction.
 \subsection*{\href{https://www.youtube.com/watch?v=hvODyYejxuc}{Project 28: The Impact of Dataset Size on Bayesian Optimization, Insights from the QM9 Dataset}}

No summary available.
 \subsection*{Project 29: A Bayesian Approach to Predict Solubility Parameters}


 \subsection*{\href{https://www.youtube.com/watch?v=78bKXIIB_GA}{Project 30: Active learning for voltammetry waveform design}}

This research project focused on using Bayesian Optimization to optimize voltammetry waveforms for sensor performance testing. The researchers employed the scikit-optimize package to implement a human-in-the-loop optimization process, allowing for pauses between optimization iterations to conduct laboratory experiments.The study encoded a four-step pulse waveform into a continuous parameter space for the optimization model. The process began with six randomly generated initial waveforms, which were tested in the lab to obtain performance metrics. These results were used to initialize a Gaussian process model. The optimization then proceeded in batches, with the model suggesting the next best waveforms to test based on previous results. The researchers conducted four total batches, updating the model after each laboratory testing phase. Progress plots were generated to visualize the improvement in sensor error across batches, demonstrating the effectiveness of the approach in finding optimal waveform parameters. The study also explored the interactions between model parameters and the performance metric across multiple dimensions, providing insights into the complex relationship between waveform characteristics and sensor performance.
 \subsection*{\href{https://www.youtube.com/watch?v=X5PDvdXoBdA}{Project 31: A tutorial on ask/tell mode for Ax}}

This project focuses on utilizing the open-source package AXS for Bayesian optimization in an asynchronous, human-in-the-loop experimental setting, particularly applied to voltammetry. The researchers developed custom figures and explanations to elucidate the complex interactions within the AXS package, emphasizing its user-friendly nature while acknowledging its potential complexity.The study explores various aspects of the optimization process, including strategy selection, surrogate model choice, and search space definition. It demonstrates two primary approaches: a "cold start" scenario, where optimization begins with no prior data, and a "warm start" scenario for situations with existing experimental data. For the cold start, the researchers showcase the use of sequence generators like Sobol sequences and random generators to initialize the optimization campaign. The project also highlights the package's ability to handle real-world experimental constraints, such as marking broken electrodes to exclude them from future suggestions. In the warm start scenario, the researchers detail the process of translating existing experimental workflows into the AXS framework, emphasizing the package's automatic tracking of experiments, results, and parameters. The study also notes the inclusion of visualization tools within AXS, though these are not described in detail.
 \subsection*{\href{https://www.youtube.com/watch?v=fxpDX7Wmdc0}{Project 32: Efficient Protein Mutagenisis using Bayesian Optimization}}

This research project explored the application of Bayesian optimization to protein mutagenesis, specifically aiming to optimize protein binding affinity to fentanyl for potential use in biosensors. The team utilized a pre-trained BERT language model, published by Andrew E. Bial, which predicts ligand binding to proteins based on amino acid sequences and SMILES strings. The methodology involved using Bayesian optimization to iteratively select positions and amino acids for mutation, starting with a protein that already had affinity to fentanyl, published by Lisa Mu Banks. The researchers focused on residues within 5 angstroms of the ligand as potential mutation sites.The team compared their Bayesian optimization approach to a baseline using random mutations. They analyzed the resulting mutations using PyRosetta's mutagenesis tool to identify potential new positive interactions and clashes that could alter protein conformation. The structural changes were further investigated using AlphaFold. While the project demonstrated the potential of Bayesian optimization for protein engineering, it was limited by its short two-day duration, suggesting room for further improvements and more extensive analysis.
 \subsection*{\href{https://www.youtube.com/watch?v=mY6Empkb8L4}{Project 33: Bayesian Optimization for Hyperspectral Co-heritability Search}}

This research project focuses on applying Bayesian Optimization (BO) to the field of crop genetics, specifically for selecting optimal proxy data to train genomic prediction models. The goal is to identify resilient crops that can withstand factors like pests and climate change. The study uses coheritability as a measure to determine the best proxy traits for desired traits, treating it as a black-box optimization problem.The researchers compared various BO methods using Gaussian Processes (GPs) against random search for four different desired traits. Surprisingly, they found that all BO methods performed similarly to random search. However, a pre-trained probabilistic model with a mixed multitask kernel outperformed standard BO approaches, identifying better proxy data more quickly. The study concludes that Gaussian Processes may not be the optimal choice for this particular problem, challenging the common use of Matérn kernels and GPs as the default in Bayesian Optimization. The researchers also criticize the widespread use of these methods without proper justification. As part of their contribution, the team has released their dataset for further research in this area.
 \subsection*{Project 34: Streamlining Material Discovery - Bayesian Optimization in Thermal Fluid Mixtures}


 \subsection*{\href{https://x.com/Ryan__Rhys/status/1820723528469262419}{Project 35: Tutorial for GAUCHE - A Library for Gaussian Processes in Chemistry}}

This research project focuses on implementing input warping for Bayesian Optimization within the Gauche library\cite{griffiths_gauche_2024}, which was previously developed by the team and published at NeurIPS 2023. The primary innovation of Gauche is the introduction of Gaussian process (GP) kernels that enable modeling of discrete entities such as SMILES strings, graphs, and bit vectors, which are common representations in molecular sciences. The motivation behind using Gaussian processes for Bayesian Optimization is their suitability for automated tasks where fine-tuning for each problem is not feasible. GPs offer a good balance between performance and simplicity, with few trainable hyperparameters that can reliably converge on each iteration of the Bayesian Optimization loop. This makes them particularly attractive as surrogate models compared to more complex alternatives like deep neural networks, which might require careful monitoring during training at each iteration. The Gauche library extends the applicability of GPs to discrete input spaces, allowing for Bayesian Optimization over molecular representations. The project team has developed a range of tutorials and applications, including molecular property prediction, protein fitness prediction, and sparse GP regression, all available in the Gauche GitHub repository.
 \subsection*{\href{https://youtu.be/CXweDiS_wbI}{Project 36: Scalable Nonmyopic Bayesian Optimization in Dynamic Cost Settings}}

This research project focuses on scalable Bayesian optimization in dynamic settings, addressing limitations of previous approaches that rely on myopic acquisition functions and assume fixed cost structures. The researchers introduce a novel method using non-myopic acquisition functions\cite{jiang_efficient_2020} that incorporate a look-ahead mechanism and dynamic cost functions. The project evaluates the proposed algorithm, named HBE, through two main experimental setups. First, they use synthetic functions across 14 different environments with varying dimensions to test scalability. Second, they apply the method to a real-world protein sequence design problem, aiming to maximize a protein score. The researchers compare their HBE algorithm against six other acquisition functions, including state-of-the-art methods. To enhance practicality, they integrate automatic hyperparameter tuning to reduce the number of optimization parameters. While specific results are not provided in the given context, the approach aims to overcome suboptimal resource allocation in dynamic cost experiments and improve upon existing Bayesian optimization techniques.
 \subsection*{\href{https://www.youtube.com/watch?v=kwXHoWV8g1E}{Project 37: The Effects of Post-Modelling Performance Metric Computation on the Efficiency of Bayesian Optimizers}}

This research project explored modifications to the Bayesian Optimization algorithm to enhance its efficiency in solving real-world problems. The key innovation was changing the order of objective calculation and model fitting. Instead of calculating all objectives first and then modeling each objective, the proposed method only models the essential observations (e.g., yield) and calculates the objectives after the modeling process. This approach was tested using a simulation of a real-world system based on kinetics discovered by Hornel, employing a plug flow reactor. The optimization aimed to maximize both space-time yield and E factor for the predicted yield.The results demonstrated significant time savings by computing objectives after modeling, primarily because only a single model for yield needed to be fitted instead of separate models for space-time yield and E factor. While computational performance improved, there was negligible impact on convergence speed. Interestingly, the research suggests that this new approach might potentially improve the efficiency of selection criteria by allowing the fitting of a prior distribution biased towards regions with more potential based on the objective function's structure. In summary, the study found that fitting objectives after modeling can lead to substantial increases in computational performance, with the possibility of also improving hypervolume convergence efficiency.
 \subsection*{\href{https://www.youtube.com/watch?v=KKxlqo15wVY}{Project 38: Bayesian methods in symbolic regression}}

This research project focused on enhancing symbolic regression methods for physical science data sets using Bayesian optimization techniques. The researchers explored two main approaches to improve the performance of symbolic regression algorithms.First, they investigated the impact of assigning varying importance to individual data points under data-limited conditions. Using the Optuna framework, they implemented a tuning process for data point importance, which resulted in improved predictive performance on a benchmark set compared to treating all points equally. This approach could be particularly useful when dealing with experimental data of uncertain quality. Second, they integrated prior knowledge from Wikipedia's known mathematical equations into the loss function, combining it with the Bayesian Information Criterion to statistically favor established symbolic structures. This method was implemented as a fitness function in the Symbolic Regression Julia library. While the researchers faced challenges in evaluating results due to numerical instability, both approaches showed promise in improving symbolic regression performance by incorporating uncertainty and prior information. The team suggests that future work should adhere more closely to the original paper's methodology, using the Bayesian information prior differences between equations as a probability metric for accepting mutational operations in the symbolic regression process.
 \subsection*{\href{https://www.youtube.com/watch?v=iog-07Ekp9g}{Project 39: Divide and Conquer - Local Gaussian Processes to design Covalent Organic Frameworks for Methane Deliverable Capacity}}

This research project focuses on improving Bayesian Optimization (BO) for high-dimensional, large-scale datasets, specifically applied to the design of Covalent Organic Frameworks (COFs) for methane storage\cite{deshwal_bayesian_2021}. The researchers developed a novel approach combining unsupervised clustering with local Gaussian Process (GP) models to enhance BO efficiency in the high data regime. The method begins by using K-means clustering to partition the dataset into distinct clusters. A portion of data from each cluster is then sampled to train separate local GP models. An epsilon-greedy algorithm is employed to determine which GP to train next. The researchers applied this approach to a COF dataset containing over 70,000 2D and 3D structures, assembled in silico from 666 organic linkers and four synthetic routes. The objective was to maximize methane storage performance, measured as deliverable capacity. The results demonstrated that their divide-and-conquer approach with local GP surrogates significantly outperformed a single GP model. While the single GP model reached a maximum deliverable capacity of 2.74 in 60 iterations, the proposed method surpassed this maximum within 5-10 iterations. Both methods retrained the GP model after 20 iterations and used 5\% of the data for initial surrogate model training. This research highlights the potential of using local GP surrogates in combination with unsupervised clustering to perform more efficient Bayesian optimization in high-dimensional, large-scale datasets.
 \subsection*{\href{https://x.com/Bozhao95501764/status/1777029207857451508}{Project 40: Optimizing Chemical Reaction Conditions with Multi-Agent Systems Using LLM and BO}}

This research project focuses on optimizing Suzuki reaction conditions using Bayesian optimization enhanced by a multi-agent large language model (LLM) system. The target dataset consists of Suzuki reactions, with the goal of optimizing reaction ligands, bases, and solvents. The researchers observed that while the target dataset contains unique combinations of conditions, many of the ligands are similar to those in existing reaction data points, suggesting potential for knowledge transfer.The developed system employs a multi-agent LLM approach comprising three domain expert agents and a reaction optimization agent. The process involves four steps: (1) the reaction optimization agent assigns tags to domain agents, (2) gathers feedback from them, (3) uses this feedback to warm-start initial data point generation and evaluation, and (4) acquires new data points based on existing data. In demo experiments, the multi-agent LLM system demonstrated improved performance, reaching target reaction yields with fewer iterations compared to traditional methods. Additionally, the multi-agent approach showed an ability to avoid local maxima traps, which is particularly valuable given the challenging nature of reaction optimization due to data sparsity. This research highlights the potential of integrating LLM-based multi-agent systems with Bayesian optimization techniques to enhance reaction condition optimization in chemistry.
 \subsection*{\href{https://x.com/6ojaHa/status/1773734082637095155}{Project 41: Retrieval Augmented Bayesian Optimization}}

This research project, called RAMBO, combines retrieval-augmented generation with Bayesian optimization to enhance the initial point selection for new optimization tasks. The system leverages existing data from literature or internal databases to identify optimal starting points for Bayesian optimization processes, particularly in the context of chemical reactions.The RAMBO pipeline begins by describing the design space of a reaction to be optimized and determining which reaction parameters should be explored first. It then queries available data to extract the most relevant data points and assembles initial suggestions for the user. The system was demonstrated using a Suzuki-Miura coupling reaction as an example, which involves an extensive combinatorial space of different parameters. RAMBO can describe the design space and limit it to available compounds, providing starting parameters to optimize the reaction. The project includes a demo interface where users can input their reaction of interest, and the React system in the backend queries and extracts relevant data to form a reply containing necessary conditions to initiate the Bayesian optimization process. Users can also explore the reasoning behind the system's suggestions by analyzing the data and reactions used to generate the final answer.
 \subsection*{Project 42: Project 42 Mqs_bodoe}


 \subsection*{Project 43: Bayesian Optimization Awesome List}


 \subsection*{\href{https://www.youtube.com/watch?v=c84Sd2IwMAQ&ab_channel=GaryTom}{Project 44: Rank-based Bayesian Optimization}}

This research project focused on using ranking models as surrogates in Bayesian optimization for materials discovery, specifically comparing ranking-based and conventional mean squared error (MSE) loss approaches. The key motivation was that in experimental campaigns, finding the best molecule is more important than accurately predicting absolute property values, inspired by work on molecular pool-based active learning\cite{graff_accelerating_2021}. The study employed a pairwise ranking loss (margin ranking loss) and a simple fully-connected multi-layer perceptron with three hidden layers and 100 nodes as the model architecture. Experiments were conducted on multiple datasets, including a solubility dataset (Delaney) and two datasets from Ali et al. with varying roughness\cite{aldeghi_roughness_2022}. Results showed that the ranking loss model consistently outperformed the MSE-based model and random baseline in Bayesian optimization, acquiring more top-performing candidates within fewer evaluations. Interestingly, all models performed better on the smoother dataset, contrary to expectations. The study also found that model performance did not always correlate with its effectiveness as a surrogate in Bayesian optimization, as evidenced by the MSE model performing worse than the random baseline on the rougher dataset. The research highlights the potential of ranking-based models in overcoming overfitting issues common in Bayesian optimization with limited data points, particularly in materials discovery applications.
 \subsection*{\href{https://www.youtube.com/watch?v=wfSyIudptfc}{Project 45: Bayesian Optimization for generality}}

This research introduces a novel approach to Bayesian Optimization (BO) focused on finding general parameters that perform well across multiple related tasks, a common challenge in fields like chemistry where optimal reaction conditions for various substrates are sought. The researchers formulate this problem within the BO framework, aiming to maximize a general response from a set of multiple related objective functions.The proposed algorithm operates in two steps: first selecting a value in the optimization domain X, then choosing an individual objective function to optimize for that value. This approach aims to find the most general values in X without exhaustively evaluating every individual surface, thus minimizing evaluation costs. To support further research in this area, the team adapted popular test surfaces (Dixon-Price, Branin, and Beale) to be compatible with their generality problem and made these benchmark problems available on Hugging Face. This contribution allows other researchers to test and develop new algorithms for generalized Bayesian Optimization, potentially advancing the field and its applications in natural sciences.
 \subsection*{\href{https://www.youtube.com/watch?v=Z7YiruHv3eE}{Multi-objective Benchmarking of Dragonfly against BoTorch}}

This research project compared the performance of two BO packages, Dragonfly\cite{JMLRdragonfly} and BoTorch\cite{balandat2020botorch}, in a multi-objective optimization context. The researchers optimized the Branin-Currin test function with added noise, conducting 20 trials of BO over different random seeds. Each trial used a batch size of 1 and consisted of 20 iterations, evaluating 20 candidates per iteration. The results showed that BoTorch's acquisition functions, particularly the q-noisy expected hypervolume improvement (qNEHVI)\cite{daulton2021parallel}, outperformed Dragonfly's acquisition function. The qNEHVI acquisition function achieved the lowest log hypervolume difference on average across the trials, while Dragonfly's acquisition function struggled to surpass a randomly generated set of candidates. Visualization of the average Pareto front produced by different acquisition functions further demonstrated BoTorch's superiority, with its acquisition functions covering a larger portion of the Pareto front compared to Dragonfly. The researchers concluded that BoTorch outperformed Dragonfly within the scope of their study, but noted the need for further investigation using different batch sizes, input and output constraints, and test functions simulating chemical reactions to apply the findings to material design problems.
 \subsection*{Bayesian Optimization Awesome List}


 \subsection*{\href{https://www.youtube.com/watch?v=fxpDX7Wmdc0}{Efficient Protein Mutagenisis using Bayesian Optimization}}

This research project explored the application of Bayesian optimization to protein mutagenesis, specifically aiming to optimize protein binding affinity to fentanyl for potential use in biosensors. The team utilized a pre-trained BERT language model, published by Andrew E. Bial, which predicts ligand binding to proteins based on amino acid sequences and SMILES strings. The methodology involved using Bayesian optimization to iteratively select positions and amino acids for mutation, starting with a protein that already had affinity to fentanyl, published by Lisa Mu Banks. The researchers focused on residues within 5 angstroms of the ligand as potential mutation sites.The team compared their Bayesian optimization approach to a baseline using random mutations. They analyzed the resulting mutations using PyRosetta's mutagenesis tool to identify potential new positive interactions and clashes that could alter protein conformation. The structural changes were further investigated using AlphaFold. While the project demonstrated the potential of Bayesian optimization for protein engineering, it was limited by its short two-day duration, suggesting room for further improvements and more extensive analysis.
 \subsection*{\href{https://x.com/6ojaHa/status/1773734082637095155}{Retrieval Augmented Bayesian Optimization}}

This research project, called RAMBO, combines retrieval-augmented generation with Bayesian optimization to enhance the initial point selection for new optimization tasks. The system leverages existing data from literature or internal databases to identify optimal starting points for Bayesian optimization processes, particularly in the context of chemical reactions.The RAMBO pipeline begins by describing the design space of a reaction to be optimized and determining which reaction parameters should be explored first. It then queries available data to extract the most relevant data points and assembles initial suggestions for the user. The system was demonstrated using a Suzuki-Miura coupling reaction as an example, which involves an extensive combinatorial space of different parameters. RAMBO can describe the design space and limit it to available compounds, providing starting parameters to optimize the reaction. The project includes a demo interface where users can input their reaction of interest, and the React system in the backend queries and extracts relevant data to form a reply containing necessary conditions to initiate the Bayesian optimization process. Users can also explore the reasoning behind the system's suggestions by analyzing the data and reactions used to generate the final answer.
 \subsection*{\href{https://www.youtube.com/watch?v=HASa3tFLZoI}{BlendDS - An intuitive specification of the design space for blends of components}}

This project focuses on developing an interface for BO that bridges the gap between domain experts (i.e. chemists and materials scientists) and machine learning algorithms. The system allows scientists to specify experimental parameters and constraints in natural language, which is then translated into a structured dictionary format by a large language model. This dictionary is subsequently converted into a Python object that can be used for design of experiments and optimization tasks.The key feature of this interface is its ability to generate diverse trial designs for efficient sampling of the experimental space. It incorporates dimensionality reduction techniques to visualize the design space, enabling users to select the most diverse and informative trials. This approach aims to maximize the information gained from each experiment, potentially reducing the number of trials needed to reach optimal results. The project is open-source, inviting contributions and integration with existing BO frameworks.
 \subsection*{\href{https://x.com/Ryan__Rhys/status/1820723528469262419}{Tutorial for GAUCHE - A Library for Gaussian Processes in Chemistry}}

This research project focuses on implementing input warping for Bayesian Optimization within the Gauche library\cite{griffiths_gauche_2024}, which was previously developed by the team and published at NeurIPS 2023. The primary innovation of Gauche is the introduction of Gaussian process (GP) kernels that enable modeling of discrete entities such as SMILES strings, graphs, and bit vectors, which are common representations in molecular sciences. The motivation behind using Gaussian processes for Bayesian Optimization is their suitability for automated tasks where fine-tuning for each problem is not feasible. GPs offer a good balance between performance and simplicity, with few trainable hyperparameters that can reliably converge on each iteration of the Bayesian Optimization loop. This makes them particularly attractive as surrogate models compared to more complex alternatives like deep neural networks, which might require careful monitoring during training at each iteration. The Gauche library extends the applicability of GPs to discrete input spaces, allowing for Bayesian Optimization over molecular representations. The project team has developed a range of tutorials and applications, including molecular property prediction, protein fitness prediction, and sparse GP regression, all available in the Gauche GitHub repository.
 \subsection*{\href{https://www.youtube.com/watch?v=KKxlqo15wVY}{Bayesian methods in symbolic regression}}

This research project focused on enhancing symbolic regression methods for physical science data sets using Bayesian optimization techniques. The researchers explored two main approaches to improve the performance of symbolic regression algorithms.First, they investigated the impact of assigning varying importance to individual data points under data-limited conditions. Using the Optuna framework, they implemented a tuning process for data point importance, which resulted in improved predictive performance on a benchmark set compared to treating all points equally. This approach could be particularly useful when dealing with experimental data of uncertain quality. Second, they integrated prior knowledge from Wikipedia's known mathematical equations into the loss function, combining it with the Bayesian Information Criterion to statistically favor established symbolic structures. This method was implemented as a fitness function in the Symbolic Regression Julia library. While the researchers faced challenges in evaluating results due to numerical instability, both approaches showed promise in improving symbolic regression performance by incorporating uncertainty and prior information. The team suggests that future work should adhere more closely to the original paper's methodology, using the Bayesian information prior differences between equations as a probability metric for accepting mutational operations in the symbolic regression process.
 \subsection*{Reliable Surrogate Models of Noisy Data}


 \subsection*{\href{https://www.youtube.com/watch?v=5f_UwsfYrc8}{BO for Drug Discovery-What is the role of molecular representation?}}

This research project investigated the impact of molecular featurization methods on BO performance for guiding molecular experiments. The team explored alternatives to the commonly used connectivity-based fingerprints, aiming to determine which featurization techniques yield the best results in BO for molecular discovery.The study found that without modifying the features, MACCS\cite{durant2002reoptimization} and RDKit\cite{landrum2013rdkit} featurization methods outperformed the default options presented in previous publications. To address the high dimensionality of molecular features, the researchers employed specialized Gaussian processes and explored random forest surrogates. However, the random forest approach did not improve performance and required more computational resources. Principal Component Analysis (PCA) was successfully used to enable Gaussian processes to handle large molecular featurizations. Notably, physicochemical featurizations like RDKit and Mordred\cite{moriwaki2018mordred} outperformed the previous benchmark winner, MolDQN\cite{zhou2019optimization}. The team's findings were consistent with benchmarks in the BO library but suggested that RDKit could produce even better results than Mordred. Overall, the research highlights the importance of careful featurization selection in BO for molecular discovery, with RDKit emerging as a particularly effective option.
 \subsection*{\href{https://www.youtube.com/watch?v=utnWbJsObF0}{Adaptive Batch Sizes for Bayesian Optimization of Reaction Yield}}

This research project investigates the optimization of batch sizes in BO for chemical reaction yield optimization. The study focuses on determining the optimal batch size (Q) at each step of the BO process, considering both smaller and larger batch sizes. The researchers examine how Q impacts the cost of retraining models, such as large language models, and how it relates to additional experimentation overhead. The project findings indicate that the optimal Q depends on a trade-off between model retraining time and additional experimentation overhead. For a fixed batch size, the researchers observed two minima at Q=3 and Q=7, reflecting the relative importance of Q in different scenarios. With very large batch sizes, the retraining cost had minimal impact on the overall experimental overhead, while experimentation overhead had a significant influence. The study also explored adaptive batch sizing strategies, decreasing batch sizes when performance was good and increasing them as the surrogate model became more trustworthy. The researchers conclude that batch sizes have a crucial effect on BO efficiency in real-world settings, and adaptive batch sizes could effectively balance the trade-off between model retraining and batch sampling effectiveness.
 \subsection*{\href{https://twitter.com/SodeAndy/status/1773474538631651769}{ScattBO Benchmark - Bayesian optimisation for materials discovery}}

This project presents ScattBO, a Python-based benchmark that simulates a self-driving laboratory (SDL) for materials discovery. A self-driving laboratory is an autonomous platform that conducts machine learning-selected experiments to achieve a user-defined objective, such as synthesizing a specific material\cite{szymanski_autonomous_2023}. The benchmark addresses the challenge that such SDLs can be expensive to run, making intelligent experimental planning essential, while only a few people have access to real SDLs for materials discovery. ScattBO provides an in silico simulation of an SDL where, based on synthesis parameters, the benchmark 'synthesizes' a structure, calculates the scattering pattern\cite{johansen_gpu-accelerated_2024}, and compares it to the target structure's scattering pattern. The benchmark acknowledges that scattering data may not be sufficient to conclusively validate that the target material has been synthesized\cite{leeman_challenges_2024}, but can include other types of data as long as they can be simulated. This makes it currently challenging to benchmark Bayesian optimization algorithms for experimental planning tasks in SDLs, and ScattBO fills this gap by providing an accessible simulation environment.
 \subsection*{\href{https://www.youtube.com/watch?v=hvODyYejxuc}{The Impact of Dataset Size on Bayesian Optimization, Insights from the QM9 Dataset}}

No summary available.
 \subsection*{Streamlining Material Discovery - Bayesian Optimization in Thermal Fluid Mixtures}


 \subsection*{\href{https://www.youtube.com/watch?v=wfSyIudptfc}{Bayesian Optimization for generality}}

This research introduces a novel approach to Bayesian Optimization (BO) focused on finding general parameters that perform well across multiple related tasks, a common challenge in fields like chemistry where optimal reaction conditions for various substrates are sought. The researchers formulate this problem within the BO framework, aiming to maximize a general response from a set of multiple related objective functions.The proposed algorithm operates in two steps: first selecting a value in the optimization domain X, then choosing an individual objective function to optimize for that value. This approach aims to find the most general values in X without exhaustively evaluating every individual surface, thus minimizing evaluation costs. To support further research in this area, the team adapted popular test surfaces (Dixon-Price, Branin, and Beale) to be compatible with their generality problem and made these benchmark problems available on Hugging Face. This contribution allows other researchers to test and develop new algorithms for generalized Bayesian Optimization, potentially advancing the field and its applications in natural sciences.
 \subsection*{\href{https://www.youtube.com/watch?v=jSPGCgH31Hc}{Robust GPs for Sustainable Concrete via Bayesian Optimization}}

This project introduces Robust Gaussian Processes (Robust GPs)\cite{altamirano2023robust} to the concrete sciences experiments with Bayesian optimization campaigns. The research addresses the common issue of outliers in concrete science experiments by applying Robust GPs to account for these anomalies. The project provides a tutorial and accompanying code in a GitHub repository, demonstrating how to implement Robust GPs in this specific context. The study utilizes a dataset provided by Meta\cite{ament2310sustainable}, which includes input variables and a target variable representing concrete strength. Initial data analysis reveals potential outliers in the strength variable, with values between 2 and 2,000. The researchers then apply Robust GP to this dataset, achieving good predictive performance. The project culminates in demonstrating how to obtain Bayesian recommendations using the Robust GP as a surrogate model, providing a practical approach for optimizing concrete mixtures while accounting for experimental outliers.
 \subsection*{\href{https://www.youtube.com/watch?v=znXZhSqFtHg}{Comparing Bayesian Optimization Methods Across Multiple Hyperparameters Against Simulated "Human" Decision-making}}

This research project investigated the effectiveness of BO methods that mimic human experimentalist approaches in materials science\cite{muckley2023interpretable}, comparing them to traditional BO techniques and random search baselines\cite{snoek2012practical,bergstra2012random}. The study focused on three key axes: model complexity, number of features, and acquisition function. The researchers found that a simplified "human experimentalist" approach, using automated feature engineering with a linear model, three features, and an exploitative acquisition function, was competitive with more complex random forest models in terms of enhancement factor and acceleration factor. Both the simplified approach and random forest models outperformed random search. Interestingly, for linear models, increasing complexity through additional features or more exploratory acquisition functions led to decreased performance. The main conclusion was that low-dimensional, interpretable models were comparable to traditional BO methods for the datasets and hyperparameter regimes studied. The researchers suggest future work could involve comparing their results to actual human performance and improving their methodology using theoretical approaches.
 \subsection*{Project 42 Mqs_bodoe}


 \subsection*{\href{https://www.youtube.com/watch?v=mY6Empkb8L4}{Bayesian Optimization for Hyperspectral Co-heritability Search}}

This research project focuses on applying Bayesian Optimization (BO) to the field of crop genetics, specifically for selecting optimal proxy data to train genomic prediction models. The goal is to identify resilient crops that can withstand factors like pests and climate change. The study uses coheritability as a measure to determine the best proxy traits for desired traits, treating it as a black-box optimization problem.The researchers compared various BO methods using Gaussian Processes (GPs) against random search for four different desired traits. Surprisingly, they found that all BO methods performed similarly to random search. However, a pre-trained probabilistic model with a mixed multitask kernel outperformed standard BO approaches, identifying better proxy data more quickly. The study concludes that Gaussian Processes may not be the optimal choice for this particular problem, challenging the common use of Matérn kernels and GPs as the default in Bayesian Optimization. The researchers also criticize the widespread use of these methods without proper justification. As part of their contribution, the team has released their dataset for further research in this area.
 \subsection*{Quantum Bayesian Optimization for Automatic Chemical Design}


 \subsection*{\href{https://www.youtube.com/watch?v=AbRDOdmafB8}{BOPE-GPT, Preference Exploration with the curious AI chemist}}

This research project focused on optimizing the Fischer-Tropsch synthesis process\cite{mahmoudi2017review}, which converts syngas into biofuel hydrocarbons. The optimization problem on the dataset\cite{lozano2008single,chakkingal2022multi} involved four input variables (space time, syngas ratio, temperature, and pressure) and four output variables (carbon conversion, selectivity, methane to paraffins, and olefins). The researchers proposed a novel approach using preferential BO\cite{lin2022preference} with large language models (LLMs) to prioritize outputs based on stakeholder preferences. The methodology employed the typical BO process powered by BoTorch, using Expected Utility as the acquisition function. However, the innovation lay in using an LLM (specifically GPT-4) to perform pairwise comparisons for output prioritization. The results showed that outputs exhibited well-defined optima trade-offs and some degree of monotonicity, which was important for preferential BO. The researchers tested three objective scenarios: optimizing all outputs equally, maximizing only CO conversion, and maximizing three objectives while minimizing olefins. For the first two objectives, the LLM-based preferential BO performed similarly to traditional utility function-based methods. However, for the third, more complex objective, the LLM approach performed worse than the utility function method and only slightly better than random exploration, suggesting limitations in the LLM's ability to handle complex optimization scenarios. The project also included the development of an interactive application for visualizing BO results and running live optimizations.
 \subsection*{\href{https://www.youtube.com/watch?v=nVtTYXxG7i4}{Bayesian Optimized De Novo Drug Design for Selective Kinase Targeting }}

This project focused on incorporating Bayesian optimization to guide de novo drug design, specifically targeting growth factor receptors for cancer therapeutics. The team built upon the DOCKSTRING paper, Python library, and dataset\cite{garcia_dockstring_2022}, using a Gaussian process with a Matérn kernel on Morgan fingerprint representations. They employed a graph genetic algorithm to generate SMILES strings guided by the Bayesian optimization output. The researchers explored both selective and promiscuous binding scenarios. For selective binding, they optimized for binding to FGFR1 while penalizing overbinding to other growth factor receptors relative to their median. For promiscuous binding, they maximized the maximum binding affinity across multiple receptors. They found that a sigmoidal penalty function was more effective than simple absolute differences when optimizing against multiple proteins. The team also incorporated a drug-likeness measure (QED)\cite{bickerton_quantifying_2012} as a penalty in the optimization process, though its effect was limited. Due to time and resource constraints, the project was unable to extensively explore the chemical space or use more accurate binding affinity calculations beyond docking. The authors suggest that future work could incorporate known unknowns through an evasion process, further optimize selective binding, and compare different molecular representations.
 \subsection*{\href{https://www.youtube.com/watch?v=WkGfShRSYW4}{Take Your Time - Improving Optimization Performance Through Greater Investment in ACQF Optimizer Runtime}}

This research project explores an enhancement to standard BO campaigns by introducing a random seed approach to improve optimization outcomes. The team from the University of Utah's Material Science and Engineering department investigated the variability in BO campaigns when starting from the same initial data points. The key innovation in this work is a custom approach to calculating the acquisition function. Instead of computing it once per iteration, as is standard in most BO methods\cite{shahriari2015taking, snoek2012practical}, the researchers calculate the acquisition function multiple times using different random seeds. They then select the highest-performing result at each iteration. This method, termed "random retries Optimizer", showed significant improvements in optimization performance, particularly in avoiding local optima. The researchers demonstrated that this approach consistently achieves near-optimal performance among the possible campaigns stemming from the initial data points. While this method requires additional computational time for the multiple acquisition function calculations, the performance gains appear to outweigh the increased computational cost. The team is continuing to investigate questions such as the trade-off between additional compute time and performance improvements, as well as how the method performs across different types of optimization problems, including simpler ones compared to the challenging example presented.
 \subsection*{\href{https://www.youtube.com/watch?v=c84Sd2IwMAQ&ab_channel=GaryTom}{Rank-based Bayesian Optimization}}

This research project focused on using ranking models as surrogates in Bayesian optimization for materials discovery, specifically comparing ranking-based and conventional mean squared error (MSE) loss approaches. The key motivation was that in experimental campaigns, finding the best molecule is more important than accurately predicting absolute property values, inspired by work on molecular pool-based active learning\cite{graff_accelerating_2021}. The study employed a pairwise ranking loss (margin ranking loss) and a simple fully-connected multi-layer perceptron with three hidden layers and 100 nodes as the model architecture. Experiments were conducted on multiple datasets, including a solubility dataset (Delaney) and two datasets from Ali et al. with varying roughness\cite{aldeghi_roughness_2022}. Results showed that the ranking loss model consistently outperformed the MSE-based model and random baseline in Bayesian optimization, acquiring more top-performing candidates within fewer evaluations. Interestingly, all models performed better on the smoother dataset, contrary to expectations. The study also found that model performance did not always correlate with its effectiveness as a surrogate in Bayesian optimization, as evidenced by the MSE model performing worse than the random baseline on the rougher dataset. The research highlights the potential of ranking-based models in overcoming overfitting issues common in Bayesian optimization with limited data points, particularly in materials discovery applications.
 \subsection*{\href{https://www.youtube.com/watch?v=RgEbcWIBDn8}{Multi-Objective Bayesian Optimization for Transparent Electromagnetic Interference Shielding with Thin-Film Structures}}

This research project focuses on applying multi-objective BO to design transparent electromagnetic interference (EMI) shielding using thin film structures\cite{li2022bayesian}. The primary objectives are to maximize both transmittance and shielding effectiveness simultaneously. The optimization problem involves selecting materials and thicknesses for each layer of the thin film structure, with a search space comprising 12 material choices (Ag, Al, Al$_2$O$_3$, Cr, Ni, Pd, Si$_3$N$_4$, SiO$_2$, Ti, TiN, TiO$_2$, W) and thickness ranges from 5 to 20 nm. The researchers implemented multi-objective BO using random scalarization for the acquisition function via BayesO package\cite{kim2023bayeso}. Gaussian process regression and expected improvement were employed as part of the BO framework. The results demonstrate that the multi-objective BO approach effectively identified the Pareto frontier for the two objectives of transmittance and shielding effectiveness. This suggests that the method is successful in finding optimal trade-offs between transparency and EMI shielding performance for thin film structures, which could have applications in areas such as spacecraft windows where both properties are crucial.
 \subsection*{\href{https://www.youtube.com/watch?v=wK266A0TvZ4}{Multiple-Context Bayesian Optimization}}

This project focused on exploring multiple context Bayesian optimization using the BAYO code, a Bayesian backend package built on BoTorch with additional features. The primary aim was to investigate transfer learning capabilities by incorporating data from existing campaigns into new optimization tasks.The researchers examined both analytical functions (like the Hartmann 3D function) and real-world data (direct arylation reaction dataset) to assess the effectiveness of transfer learning in Bayesian optimization. They introduced noise, scaling, shifting, and negation to the analytical functions to simulate realistic scenarios. The results demonstrated that using a small percentage (1-25\%) of existing data significantly improved optimization performance compared to the baseline without transfer learning. Interestingly, they found that using larger amounts of data (50-100\%) did not necessarily lead to better results and in some cases performed worse than the baseline. The team also explored clustering the existing data and using cluster centroids as source data, which proved effective. For the real-world direct arylation dataset, they analyzed correlations between different reaction temperatures and observed that small amounts of transfer data (1-10\%) substantially improved the Bayesian optimization campaign. Overall, the project highlighted the potential of transfer learning in Bayesian optimization while emphasizing that careful selection of the amount and type of transfer data is crucial for optimal performance.
 \subsection*{\href{https://www.youtube.com/watch?v=Qr2cz5lxM64&ab_channel=ArifinSan}{SimpleGPT-BO, Simplified GPT-Powered Bayesian Optimization}}

No summary available.
 \subsection*{\href{https://youtu.be/CXweDiS_wbI}{Scalable Nonmyopic Bayesian Optimization in Dynamic Cost Settings}}

This research project focuses on scalable Bayesian optimization in dynamic settings, addressing limitations of previous approaches that rely on myopic acquisition functions and assume fixed cost structures. The researchers introduce a novel method using non-myopic acquisition functions\cite{jiang_efficient_2020} that incorporate a look-ahead mechanism and dynamic cost functions. The project evaluates the proposed algorithm, named HBE, through two main experimental setups. First, they use synthetic functions across 14 different environments with varying dimensions to test scalability. Second, they apply the method to a real-world protein sequence design problem, aiming to maximize a protein score. The researchers compare their HBE algorithm against six other acquisition functions, including state-of-the-art methods. To enhance practicality, they integrate automatic hyperparameter tuning to reduce the number of optimization parameters. While specific results are not provided in the given context, the approach aims to overcome suboptimal resource allocation in dynamic cost experiments and improve upon existing Bayesian optimization techniques.
 \subsection*{\href{https://www.youtube.com/watch?v=Cyyj9ySybZE}{Interpretability of Bayesian Optimisation Campaigns}}

This research project focused on developing novel methods for interpreting Bayesian Optimization campaigns by incorporating the temporal component often neglected in end-of-campaign analyses. The study utilized data from a self-driving lab experiment aimed at optimizing the conductivity of a coating. The researchers recreated the Gaussian Process (GP) model employed in the original experiment, training it on subsets of data to simulate mid-campaign conditions. Two primary methods were investigated: First, cross-sections of the GP model were taken at various stages of the campaign, holding all but one feature constant. This approach revealed that closer nozzle distances produced better results, aligning with prior beliefs and serving as a potential mid-campaign sense check to validate model alignment with previous experimentation. The second method involved predicting the error of the next sample based on the model trained on data collected up to that point. This analysis showed that performance improvement plateaued around the time the optimal sample was produced, suggesting its potential use as an early stopping criterion if error is monitored throughout the campaign.The research demonstrates the value of incorporating temporal analysis in Bayesian Optimization campaigns, offering insights that could enhance decision-making during experiments. The cross-sectional approach provides a means to validate model behavior against prior knowledge, while the error prediction method could inform stopping criteria, potentially improving efficiency in optimization processes. These techniques offer promising avenues for real-time interpretation and guidance in Bayesian Optimization experiments, particularly in self-driving lab contexts.
 \subsection*{\href{https://www.youtube.com/watch?v=iog-07Ekp9g}{Divide and Conquer - Local Gaussian Processes to design Covalent Organic Frameworks for Methane Deliverable Capacity}}

This research project focuses on improving Bayesian Optimization (BO) for high-dimensional, large-scale datasets, specifically applied to the design of Covalent Organic Frameworks (COFs) for methane storage\cite{deshwal_bayesian_2021}. The researchers developed a novel approach combining unsupervised clustering with local Gaussian Process (GP) models to enhance BO efficiency in the high data regime. The method begins by using K-means clustering to partition the dataset into distinct clusters. A portion of data from each cluster is then sampled to train separate local GP models. An epsilon-greedy algorithm is employed to determine which GP to train next. The researchers applied this approach to a COF dataset containing over 70,000 2D and 3D structures, assembled in silico from 666 organic linkers and four synthetic routes. The objective was to maximize methane storage performance, measured as deliverable capacity. The results demonstrated that their divide-and-conquer approach with local GP surrogates significantly outperformed a single GP model. While the single GP model reached a maximum deliverable capacity of 2.74 in 60 iterations, the proposed method surpassed this maximum within 5-10 iterations. Both methods retrained the GP model after 20 iterations and used 5\% of the data for initial surrogate model training. This research highlights the potential of using local GP surrogates in combination with unsupervised clustering to perform more efficient Bayesian optimization in high-dimensional, large-scale datasets.
 \subsection*{\href{https://www.youtube.com/watch?v=I179UR8P054}{Chemical Similarity-Informed Earth Mover’s Distance Kernel Bayesian Optimization for Predicting the Properties of Molecules and Molecular Mixtures}}

This research project focuses on developing chemical similarity-informed distance functions and kernels for explainable Bayesian optimization, specifically targeting the prediction of properties for molecular mixtures. The researchers propose a novel approach that bypasses the need for embedding vectors by directly providing pairwise distances between data points in the kernel function of a Gaussian Process (GP) model\cite{moss_gaussian_2020}. The project introduces the Earth Mover's Distance (EMD) kernel\cite{hargreaves_earth_2020} into the GP framework to calculate pairwise distances between mixtures based on individual component distances. This method was tested for predicting yields of binary reactant mixtures, demonstrating high chemical resolution in mixture analysis. The results show that the EMD kernel achieves accurate yield predictions with narrow distributions for both high and low-yield cases, indicating improved performance in distinguishing between different mixture compositions. By incorporating smooth distance metrics, the researchers successfully extended Bayesian optimization techniques from pure components to molecular mixtures, potentially enhancing the efficiency and interpretability of materials property prediction in complex chemical systems.
 \subsection*{\href{https://youtu.be/Qbvq7uolQr8}{Closed loop optimization of hydrogel formulations using dynamic light scattering}}

This research project proposes a self-driving lab approach to optimize hydrogel formulations using BO and automated characterization techniques. The study aims to address the challenge of creating hydrogels with specific properties for applications such as cell culture, tissue engineering, drug delivery, and agriculture. The proposed workflow utilizes an open-source liquid handling robot to mix hydrogel formulations in a 96-well plate, followed by cross-linking using various methods (LED array, heating module, or time). The key innovation is the use of Dynamic Light Scattering (DLS)\cite{stetefeld2016dynamic} for automated characterization of gelation and viscoelastic properties, which can measure gel stiffness up to 10 kilopascals. This DLS method is implemented using a plate reader specifically designed for 96-well plates, enabling high-throughput analysis. The resulting property data is then fed into a Bayesian Optimizer to determine the next set of formulations and processing parameters to test. This approach aims to efficiently explore the complex relationship between hydrogel formulation parameters and their resulting properties, potentially accelerating the development of custom hydrogels for specific applications in biological research and improving data availability and reliability in the field.
 \subsection*{Bayesian optimization of likely negative candidates in imbalanced biological datasets}


 \subsection*{\href{https://www.youtube.com/watch?v=kIRxGdwmLSY}{BayBE One More Time - Exploring Corrosion Inhibitors for Materials Design}}

This research project explored the use of BO for efficiently identifying effective corrosion inhibitors in materials design. The researchers utilized the BO implementation from the BayBE package\cite{fitzner2022baybe} to evaluate its performance on multiple experimental datasets\cite{galvao2022cordata} involving different alloys, primarily aluminum-based. The study found that the choice of molecular descriptor encoding significantly impacted the optimization performance, especially for larger datasets like A1000 and A2024. The SMILES-based approach with Mordred encoding\cite{moriwaki2018mordred} outperformed the other methods for these datasets, while random sampling underperformed. Interestingly, for the A775 dataset, which had a sparse distribution of reported efficiencies and fewer data points, this trend was not observed. The researchers also explored transfer learning capabilities, using information from the A2024 alloy dataset to inform the optimization process for the A1000 dataset. This transfer learning approach outperformed the uninformed method after only 15 iterations, demonstrating its potential for accelerating the discovery of effective corrosion inhibitors in experimental settings.
 \subsection*{\href{https://www.youtube.com/watch?v=X5PDvdXoBdA}{A tutorial on ask/tell mode for Ax}}

This project focuses on utilizing the open-source package AXS for Bayesian optimization in an asynchronous, human-in-the-loop experimental setting, particularly applied to voltammetry. The researchers developed custom figures and explanations to elucidate the complex interactions within the AXS package, emphasizing its user-friendly nature while acknowledging its potential complexity.The study explores various aspects of the optimization process, including strategy selection, surrogate model choice, and search space definition. It demonstrates two primary approaches: a "cold start" scenario, where optimization begins with no prior data, and a "warm start" scenario for situations with existing experimental data. For the cold start, the researchers showcase the use of sequence generators like Sobol sequences and random generators to initialize the optimization campaign. The project also highlights the package's ability to handle real-world experimental constraints, such as marking broken electrodes to exclude them from future suggestions. In the warm start scenario, the researchers detail the process of translating existing experimental workflows into the AXS framework, emphasizing the package's automatic tracking of experiments, results, and parameters. The study also notes the inclusion of visualization tools within AXS, though these are not described in detail.
 \subsection*{\href{https://x.com/Bozhao95501764/status/1777029207857451508}{Optimizing Chemical Reaction Conditions with Multi-Agent Systems Using LLM and BO}}

This research project focuses on optimizing Suzuki reaction conditions using Bayesian optimization enhanced by a multi-agent large language model (LLM) system. The target dataset consists of Suzuki reactions, with the goal of optimizing reaction ligands, bases, and solvents. The researchers observed that while the target dataset contains unique combinations of conditions, many of the ligands are similar to those in existing reaction data points, suggesting potential for knowledge transfer.The developed system employs a multi-agent LLM approach comprising three domain expert agents and a reaction optimization agent. The process involves four steps: (1) the reaction optimization agent assigns tags to domain agents, (2) gathers feedback from them, (3) uses this feedback to warm-start initial data point generation and evaluation, and (4) acquires new data points based on existing data. In demo experiments, the multi-agent LLM system demonstrated improved performance, reaching target reaction yields with fewer iterations compared to traditional methods. Additionally, the multi-agent approach showed an ability to avoid local maxima traps, which is particularly valuable given the challenging nature of reaction optimization due to data sparsity. This research highlights the potential of integrating LLM-based multi-agent systems with Bayesian optimization techniques to enhance reaction condition optimization in chemistry.
 \subsection*{A Bayesian Approach to Predict Solubility Parameters}


 \subsection*{\href{https://www.youtube.com/watch?v=kwXHoWV8g1E}{The Effects of Post-Modelling Performance Metric Computation on the Efficiency of Bayesian Optimizers}}

This research project explored modifications to the Bayesian Optimization algorithm to enhance its efficiency in solving real-world problems. The key innovation was changing the order of objective calculation and model fitting. Instead of calculating all objectives first and then modeling each objective, the proposed method only models the essential observations (e.g., yield) and calculates the objectives after the modeling process. This approach was tested using a simulation of a real-world system based on kinetics discovered by Hornel, employing a plug flow reactor. The optimization aimed to maximize both space-time yield and E factor for the predicted yield.The results demonstrated significant time savings by computing objectives after modeling, primarily because only a single model for yield needed to be fitted instead of separate models for space-time yield and E factor. While computational performance improved, there was negligible impact on convergence speed. Interestingly, the research suggests that this new approach might potentially improve the efficiency of selection criteria by allowing the fitting of a prior distribution biased towards regions with more potential based on the objective function's structure. In summary, the study found that fitting objectives after modeling can lead to substantial increases in computational performance, with the possibility of also improving hypervolume convergence efficiency.
 \subsection*{\href{https://www.youtube.com/watch?v=4gPTMaarQt0}{How does initial warm-up data influence Bayesian optimization in low-data experimental settings?}}

This research project investigated the influence of warm-up sampling methods and dataset sizes on property optimization in low data regimes, specifically focusing on molecular property prediction. The team used the QM9 dataset\cite{ramakrishnan_quantum_2014} and selected band gap as the optimization target. They compared two chemically-inspired sampling methods for the warm-up dataset: Morgan fingerprints and MolFormer language model fingerprints. The researchers also referenced the GDB-17 chemical universe database\cite{ruddigkeit_enumeration_2012} in their background work. The researchers performed dimensionality reduction on the fingerprints using PCA, projecting them into a 2D space for sampling. They conducted experiments to analyze how the warm-up dataset size affects optimization results. The most significant finding was the comparison between Morgan fingerprints and MolFormer fingerprints at a constant data regime of 50 data points. The results showed that MolFormer fingerprints substantially outperformed Morgan fingerprints, suggesting that pre-trained models on large chemical spaces can potentially improve model optimization rates. This study aims to initiate broader discussions on how dataset sizes and sampling methodologies impact final optimization tasks in molecular property prediction.
 \subsection*{\href{https://www.youtube.com/watch?v=78bKXIIB_GA}{Active learning for voltammetry waveform design}}

This research project focused on using Bayesian Optimization to optimize voltammetry waveforms for sensor performance testing. The researchers employed the scikit-optimize package to implement a human-in-the-loop optimization process, allowing for pauses between optimization iterations to conduct laboratory experiments.The study encoded a four-step pulse waveform into a continuous parameter space for the optimization model. The process began with six randomly generated initial waveforms, which were tested in the lab to obtain performance metrics. These results were used to initialize a Gaussian process model. The optimization then proceeded in batches, with the model suggesting the next best waveforms to test based on previous results. The researchers conducted four total batches, updating the model after each laboratory testing phase. Progress plots were generated to visualize the improvement in sensor error across batches, demonstrating the effectiveness of the approach in finding optimal waveform parameters. The study also explored the interactions between model parameters and the performance metric across multiple dimensions, providing insights into the complex relationship between waveform characteristics and sensor performance.
 \subsection*{\href{https://www.youtube.com/watch?v=5AjwoZtjgOc}{Comparative Analysis of Acquisition Functions in Bayesian Optimization for Drug Discovery}}

This project investigates the application of BO techniques directly on molecular fingerprints for drug discovery, focusing on comparing different surrogate models, acquisition functions and small, diverse, unbalanced, and noisy datasets\cite{bellamy2022batched}. The researchers used the LD50 dataset from PTDC with SMILES encodings\cite{wu2018quantitative,chen2021algebraic}, which were transformed into ECFP fingerprints\cite{rogers2010extended}, creating a 2048-dimensional feature space. Two surrogate models were examined: Gaussian Processes (GP) and Random Forests (RF). The results showed that in this high-dimensional space, Random Forests with uncertainties evaluated as variance between different trees achieved expected performance across all acquisition functions, significantly outperforming random selection. In contrast, Gaussian Processes failed to perform well, likely due to the challenges posed by the high-dimensional space ($2\textasciicircum{}{2048}$ possible combinations) and the relatively small dataset of only 7,000 data points. The researchers concluded that machine learning models, particularly Random Forests with various acquisition functions, perform well when dealing with high-dimensional molecular fingerprint spaces, demonstrating their potential for drug discovery applications with certain selection biases.
 \subsection*{\href{https://www.youtube.com/watch?v=uYXAe3sRUSo}{Benchmarking Molecular Descriptors with Actively Identified Subsets (MolDAIS)}}

This research presents a novel approach called MOLDES (Molecular Descriptors with Actively Identified Subspaces) for molecular property optimization. The method addresses the challenge of optimizing molecules in high-dimensional spaces by using molecular descriptors - sets of rotationally and translationally invariant calculations performed on molecular graphs - coupled with active subspace identification. MOLDES employs a sparse axis-aligned subspace Gaussian Process prior, which actively learns an encoding while performing Bayesian optimization. Recent works\cite{sorourifar_accelerating_2024,maus_local_2023} are increasingly turning towards active encoding of molecular feature spaces. The researchers evaluated MOLDES on three case studies: experimental lipophilicity (4,200 compounds), log P optimization benchmark (250,000 molecules), and power conversion efficiency from the Harvard Clean Energy Project (30,000 compounds). In all cases, MOLDES demonstrated superior performance compared to other optimizers, particularly in larger datasets. For the log P optimization, MOLDES consistently found the optimal molecule within 100 iterations. The method also showed strong performance in constrained optimization problems, often achieving the best-case scenario and maintaining a favorable worst-case scenario compared to other methods. Overall, MOLDES proved efficient in identifying high-performing molecules in low-data regimes, offering a promising approach for molecular property optimization tasks.
 \subsection*{\href{https://youtu.be/l0aVZDMwIMU}{Optimal MOF Selection for CO$_2$ capture using Thompson sampling}}

This research project focuses on optimizing the selection of Metal-Organic Frameworks (MOFs) for carbon capture applications using BO, specifically Thompson Sampling. MOFs are nanoporous materials with high potential for carbon capture\cite{furukawa2013chemistry, heo2020metal}, but their synthesis and characterization are expensive and time-consuming\cite{desantis2017techno}. The goal was to develop an efficient method for identifying high-performing MOF candidates using a small dataset. The team employed a Gaussian Process model trained on the CRAFTED dataset\cite{oliveira2023crafted}, using both RACs features and geometric features of MOFs, with CO$_2$ uptake as the output. Thompson Sampling was used as the acquisition function to select the next best sample based on the posterior distribution, inherently performing a Bayesian update. The results showed that Thompson Sampling was twice as efficient as random sampling in identifying high-performing MOF candidates. This approach is notable for being the first application of Thompson Sampling to MOF candidate selection, and it requires no hyperparameter tuning, making it easily transferable and cost-efficient. The potential impact of this method is significant, as it could accelerate MOF design for direct air capture applications, with a goal of absorbing 30 million tons of CO$_2$, and could also be applied to post-combustion capture scenarios.
 \subsection*{Investigation of Multi-Objective Bayesian Optimization of QM9 Dataset}


 \subsection*{\href{https://www.youtube.com/watch?v=pegJumJEOsE}{Long-run Behaviour of Multi-fidelity Bayesian Optimisation}}

This research project investigates the long-term performance of multifidelity Bayesian optimization (MFBO) methods\cite{poloczek2017multi} compared to single-fidelity Bayesian optimization (SFBO) methods. The study focuses on scenarios where two fidelities are available: an accurate but expensive target fidelity that fully represents the objective, and a less accurate but cheaper low fidelity. The researchers observed that while MFBO methods initially outperform SFBO up to a budget of approximately 20, they begin to underperform beyond this point. This behavior is particularly evident with linear kernel MFBO methods, where the performance gap becomes more pronounced after a cost of around 60.The project aims to evaluate the factors contributing to the long-term issues of MFBO methods. Specifically, the researchers plan to investigate the impact of kernel choice, frequency of low fidelity queries, and acquisition function selection on MFBO performance. By analyzing these aspects, the study seeks to identify potential improvements for MFBO methods to enhance their applicability in real-life optimization tasks. The ultimate goal is to develop MFBO approaches that can maintain their performance advantage over SFBO methods in the long run, leveraging the cost-effectiveness of low fidelity queries while achieving superior optimization results.
 \subsection*{\href{https://www.youtube.com/watch?v=4lFEUixwkE8}{Navigating the black box of zeolite synthesis with Bayesian Optimization}}

This research project explores the application of BO to zeolite synthesis, a complex process with significant industrial importance. Zeolites are crystalline materials composed of interconnected silicate and aluminate tetrahedra, widely used as absorbents and catalysts\cite{dusselier2018small}. The synthesis of zeolites involves multiple parameters, including silicon and aluminum sources, organic molecules, water, temperature, and time. The process aims to achieve specific properties such as high crystallinity, large external surface area, and particular silicon-to-aluminum ratios, while also considering economic factors like synthesis temperature, duration, and ingredient concentrations\cite{mallette2024current}. The project presents a GitHub repository containing an introductory text on zeolites and their synthesis, along with a Jupyter notebook demonstrating BO applications. The notebook is divided into two main sections: the first optimizes an analytical dummy function using zeolite synthesis parameters, exploring various aspects of BO including continuous, categorical, and mixed variable types, parameter constraints, and single and multiple objectives. The second section applies BO to propose new experiments based on existing literature data. This approach aims to accelerate the traditionally time-consuming trial-and-error process of zeolite synthesis optimization, potentially reducing costs and improving efficiency in industrial applications.

